---
layout: post
title: "Stellar Classification - SDSS17"
date: 2023-10-9
author: Juan Martin Maidana
categories: [Clasificación]
tags: [Remuestreo, Visualización de Datos, Clasificación Multiclase]
image:
  path: /assets/img/favicons/stellar/preview.jpg
  alt: Stellar Preview
---


# Stellar Classification - SDSS17

Uno de los temas que siempre han fascinado es el espacio. La inmensidad del universo, los misterios de las galaxias y la belleza del cielo nocturno siempre han mantenido la curiosidad de la humanidad. En el mundo de la astronomía, una de las tareas fundamentales es la clasificación de los objetos celestes, y el "Stellar Classification Dataset - SDSS17" ofrece una oportunidad apasionante para explorar y comprender las características espectrales de estrellas, galaxias y cuásares. Este conjunto de datos, basado en observaciones del Sloan Digital Sky Survey, contiene mucha información que nos permite clasificar estos objetos y adentrarnos en las maravillas de nuestro cosmos. Embarquémonos en este viaje astronómico y analicemos este conjunto de datos.


## Metadata Dataset

### Contexto - SDSS17

![StellarSDSS](/assets/img/favicons/stellar/sdss.jpg){: width="972" height="589" .w-50 .right}

El Sloan Digital Sky Survey (SDSS), a  frecuentemente denominado simplemente SDSS, constituye un logro monumental en el campo de la astronomía, pionero en desvelar los secretos de nuestro vasto cosmos. Este estudio multiespectral de imágenes y corrimientos al rojo espectroscópicos se lleva a cabo con un telescopio óptico gran angular de 2,5 metros situado en el Observatorio de Apache Point, en Nuevo México (Estados Unidos). Desde su creación en el año 2000, el SDSS ha transformado radicalmente nuestra comprensión del universo.

En esencia, el SDSS se basó en una instrumentación y unas técnicas de procesamiento de datos revolucionarias. Utilizaba una cámara CCD de barrido con múltiples filtros y matrices para capturar imágenes de alta calidad del cielo nocturno. Estas imágenes se complementaban con un espectrógrafo multiobjeto/multifibra capaz de obtener simultáneamente espectros de numerosos objetos celestes.

![StellarSDSSFilters](/assets/img/favicons/stellar/filters.jpg){: width="972" height="589" .w-50 .left}


Uno de los principales retos del proyecto fue hacer frente a la gran catidad de volumen de datos generados cada noche por el telescopio y los instrumentos. Con el tiempo, estos datos se procesaron y dieron lugar a la creación de extensos catálogos de objetos astronómicos, disponibles para su consulta digital. El SDSS desempeñó un papel fundamental en el avance de las tecnologías de almacenamiento y accesibilidad de bases de datos masivas, haciendo que sus datos fueran ampliamente accesibles a través de Internet, un concepto relativamente novedoso en aquella época.



La cámara de imágenes recoge datos de imágenes fotométricas utilizando un conjunto de 30 CCD SITe/Tektronix de 2048 por 2048 píxeles colocados en seis columnas de cinco CCD cada una, alineadas con las columnas de píxeles de los propios CCD. Los filtros r, i, u, z y g del SDSS cubren las filas respectivas del conjunto, en ese orden. El sondeo opera el instrumento en modo de barrido a la deriva: la cámara lee lentamente los CCDs a medida que se recogen los datos, mientras el telescopio se mueve a lo largo de grandes círculos en el cielo, de modo que las imágenes de los objetos se mueven a lo largo de las columnas de los CCDs a la misma velocidad a la que se leen los CCDs.


### Contenido - SDSS17

El conjunto de datos comprende 100.000 observaciones de objetos celestes captadas por el Sloan Digital Sky Survey (SDSS). Cada observación se caracteriza por 17 columnas de características y 1 columna de clase, que clasifica el objeto como estrella, galaxia o cuásar.

He aquí una descripción detallada de las columnas del conjunto de datos:

- obj_ID (Identificador de objeto): Se trata de un identificador único asignado a cada objeto en el catálogo de imágenes utilizado por el CAS (Catalog Archive Server).

- alpha (Ángulo de ascensión recta): El ángulo de ascensión recta, medido en la época J2000, especifica la posición este-oeste del objeto en el cielo.

- delta (ángulo de declinación): El ángulo de declinación, medido en la época J2000, especifica la posición norte-sur del objeto en el cielo.

- u (Filtro ultravioleta): El filtro u corresponde a la porción ultravioleta del sistema fotométrico y proporciona información sobre el brillo del objeto en este rango.

- g (Filtro verde): El filtro g corresponde a la parte verde del sistema fotométrico y mide el brillo del objeto en la longitud de onda verde.


- r (Filtro rojo): El filtro r corresponde a la parte roja del sistema fotométrico y mide el brillo del objeto en la longitud de onda del rojo.

- i (Filtro infrarrojo cercano): El filtro i corresponde a la porción infrarroja cercana del sistema fotométrico y mide el brillo del objeto en este rango.

- z (Filtro infrarrojo): El filtro z corresponde a la porción infrarroja del sistema fotométrico y proporciona información sobre el brillo del objeto en la longitud de onda infrarroja.

- run_ID (Número de ejecución): Se trata de un identificador único utilizado para especificar la exploración concreta durante la cual se realizó la observación.

- rereun_ID (Número de repetición): El número de repetición especifica cómo se procesó la imagen, indicando cualquier reprocesamiento o modificación aplicada a los datos.

- cam_col (Columna de cámara): El número de columna de la cámara identifica la línea de barrido específica dentro de la ejecución.

- field_ID (Número de campo): El número de campo se utiliza para identificar cada campo, que es una región específica del cielo.

- spec_obj_ID (Identificador de objeto espectroscópico): Se trata de un identificador único para objetos espectroscópicos ópticos. Las observaciones con el mismo spec_obj_ID comparten la misma clase de salida.

- class (Clase de objeto): Esta columna categoriza el tipo de objeto celeste observado, incluyendo galaxias, estrellas o cuásares.

- redshift: El valor del desplazamiento al rojo se basa en el aumento de la longitud de onda y es una medida fundamental para comprender el movimiento y la distancia de los objetos en el universo.

- plate (Plate ID): El ID de placa identifica cada placa utilizada en el SDSS. Las placas son objetos físicos con agujeros perforados para permitir que las fibras recojan la luz de los objetos celestes.

- MJD (Fecha Juliana Modificada): La MJD se utiliza para indicar cuándo se recogió un dato concreto del SDSS, proporcionando una marca de tiempo precisa para cada observación.

- fiber_ID (ID de fibra): El ID de fibra identifica la fibra específica utilizada para apuntar la luz al plano focal durante cada observación. Esto permite al SDSS recoger luz de diferentes objetos simultáneamente.


## Input data

El primer paso a realizar es importar nuestras librerías que utilizaremos a lo largo del documento, así como comprobar si se han cargado correctamente imprimiendo las primeras líneas del conjunto de datos.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neighbors import LocalOutlierFactor
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import MinMaxScaler
from yellowbrick.classifier import ConfusionMatrix
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split


from sklearn.model_selection import StratifiedKFold
fold = StratifiedKFold(n_splits=4, shuffle = True, random_state=42)

from sklearn.metrics import recall_score

from warnings import filterwarnings
filterwarnings(action='ignore')
```

```python
df = pd.read_csv("star_classification.csv")
df.head()
```

![StellarHead](/assets/img/favicons/stellar/head.png)

Aquí encontrará información adicional sobre este conjunto de datos, filas, información estadística, entre otros.

```python
df.shape
```

![StellarShape](/assets/img/favicons/stellar/shape.png)

### Ajustando la variable objetivo

```python
df.info()
```

![StellarInfo](/assets/img/favicons/stellar/info.png){: width="1172" height="889" .w-75}

La observación al ver estos datos es que la etiqueta es de tipo objeto, algo que cambiamos a numérico para poder utilizarlo en modelos en el futuro.

```python
df["class"]=[0 if i == "GALAXY" else 1 if i == "STAR" else 2 for i in df["class"]]
```

Con este código cambiaremos "GALAXY" por el número 0, "STAR" por el número 1, y "Quasars" por el número 2. También podemos ver en la tabla de información que en 100.000 filas no falta ningún valor, una gran noticia ya que nos facilitará el trabajo de preparacion de datos.

### Outliers

Cuando nos enfrentamos a los valores atípicos en un conjunto de datos tan grande, tenemos que ser más cuidadosos, por eso se decido utilizar un enfoque más robusto, éste algorimo fue el Local Outlier Factor (LOF). La idea del LOF es comparar cada punto de datos con sus vecinos cercanos. Calcula a qué distancia está cada punto de sus vecinos y comprueba si un punto está mucho más lejos de sus vecinos de lo que están entre sí. Si está mucho más lejos, se considera una anomalía. Si no está mucho más lejos, se considera un punto normal. LOF utiliza esta comparación de densidad relativa para encontrar anomalías locales en un conjunto de datos.

```python
clf = LocalOutlierFactor()
y_pred = clf.fit_predict(df)

x_score = clf.negative_outlier_factor_
outlier_score = pd.DataFrame()
outlier_score["score"] = x_score

#threshold
threshold2 = -1.5
filtre2 = outlier_score["score"] < threshold2
outlier_index = outlier_score[filtre2].index.tolist()

len(outlier_index)
```

```python
df.drop(outlier_index, inplace=True)
df.shape
```

![StellarOutliers](/assets/img/favicons/stellar/shapeout.png)


Después de eliminar los valores atípicos con LOF obtenemos un total de 84744 filas en 18 características, ahora es el momento de echar un vistazo más de cerca a los atributos.


## Visualización de Data

Mirando a la variable objetivo podemos ver que los datos están realmente desequilibrados, casi el 60% de las filas son galaxias, y el 19% son Cuásares. Este es un problema que vamos a atacar en el futuro, en el momento de entrenar un modelo, pero por ahora vamos a echar un vistazo a las otras características.

![StellarLabel](/assets/img/favicons/stellar/label.png)

Una vez que analizamos un poco la etiqueta, ahora podemos centrarnos en las características, en este gráfico analizamos la distribución y densidad de cada característica en relación con la etiqueta objetivo. Aunque este gráfico es exhaustivo, proporciona información valiosa. En el lado izquierdo, puede ver las densidades de las características representadas por los gráficos de estimación de densidad de Kernel (KDE). Estos gráficos muestran cómo varía la distribución de cada característica en las tres clases y proporcionan una visión agregada de todas las clases. A la derecha, los histogramas ilustran las mismas distribuciones. Esta visualización es muy útil para evaluar la importancia de las características y las relaciones lineales con la etiqueta.

## Feature Selection

```python
# Select the features you want to visualize in the plots
features = df.drop('class', axis=1)

# Mapping of real labels to desired labels
label_mapping = {0: 'Galaxy', 1: 'Star', 2: 'Quasar'}

# Create a grid of subplots
fig, axes = plt.subplots(nrows=len(features.columns), ncols=2, figsize=(12, 2*len(features.columns)))
fig.subplots_adjust(hspace=0.5)

# Iterate through the features and generate plots
for i, col in enumerate(features.columns):
    # KDE Plot
    ax1 = axes[i, 0]
    for label_value, label_name in label_mapping.items():
        sns.kdeplot(data=df[df["class"] == label_value][col], label=label_name, ax=ax1)
    sns.kdeplot(data=df[col], label="All", ax=ax1)
    ax1.set_title(f'{col} KDE')
    ax1.legend()

    # Histogram Plot
    ax2 = axes[i, 1]
    for label_value, label_name in label_mapping.items():
        sns.histplot(data=df[df["class"] == label_value][col], bins=25, color='blue', alpha=0.7, ax=ax2, label=label_name)
    sns.histplot(data=df[col], bins=25, color='red', alpha=0.5, ax=ax2, label="All")
    ax2.set_title(f'{col} Histogram')
    ax2.legend()

plt.tight_layout()
plt.show()

```


![StellarDesc](/assets/img/favicons/stellar/obus2.png)


Centrándonos más en los gráficos de densidad, tenemos que analizar la relación lineal con la etiqueta, por ejemplo, en "obj_id" podemos ver que para cualquier valor de "obj_id" la etiqueta no tiene ninguna varianza. La densidad en diferentes valores de "obj_id" es siempre la misma para las 3 clases (las 3 clases siguen la misma tendencia), de esto podemos deducir que esta característica podría ser  inútil para la varianza de la etiqueta.

Podemos ver algo similar para "alpha" y "beta", sabemos que estos atributos definen la ubicación donde se encuentra el objeto celeste en el espacio, y con estos gráficos podemos decir que la ubicación del objeto celeste no es una característica importante para definir si se trata de una galaxia, estrella o cuásar.

Hay algunas características que siguen esta tendencia ("run_Id", "cam_col", "field_Id", entre otras), algunas de ellas porque son sólo identificadores o características no útiles para el modelo pero importantes para algunos astrólogos.

Pero algunas características que podemos ver claramente que pueden ser importantes para la etiqueta, por ejemplo: "u", "g", "r", "i", "z" y "plate". Por ejemplo, en el caso de "plate" podemos ver muchas variaciones a lo largo de los valores de "plate", desde valores de 2000 a 4000 podemos ver que tenemos altas probabilidades de que sea una estrella, y de 7000 a 9500 podría ser un cuásar. Esto se puede ver en las características mencionadas anteriormente.

Observando la matriz de correlaciones obtenemos los siguientes resultados.

![StellarMatrix](/assets/img/favicons/stellar/matrix.png)

Como se ha dicho, algunas de las características que ya hemos mencionado son las que están más correlacionadas, pero "redshift" tiene la correlación lineal más alta entre todas las características, esto significa, que tiene mucho peso en la decisión de la etiqueta con un 0,54, echando un vistazo más de cerca a esta característica obtenemos el siguiente gráfico.

>  `0` para Galaxia, `1` for Estrella, `2` for Quasar.
{: .prompt-tip }


![StellarRedshift](/assets/img/favicons/stellar/redshift.png){: width="972" height="589" .w-50 .right}


Ahora que miramos más de cerca, podemos ver claramente la diferencia en densidad para diferentes valores del "redshift", de -15 a -5 podemos ver que la densidad se dispara para Estrella, mientras que de -3 a -0.5 parece ser una Galaxia, y para valores superiores a 1, es probable que sea un Cuásar.


En este otro gráfico se puede ver fácilmente la separación entre el valor de desplazamiento al rojo y las clases.


![StellarRedshif2](/assets/img/favicons/stellar/redshift2.png)


Basándonos en estos gráficos podemos concluir que existe una fuerte correlación lineal entre el redshift y la etiqueta

A partir de estos gráficos se concluye que características vamos a utilizar en los modelos, nos deshacemos de otras ya que hay ids u otros atributos que no tienen casi correlación con la variable.




## Datos Desequilibrados


Como observamos en el gráfico anterior, que representa las etiquetas, es evidente que nuestro conjunto de datos sufre un desequilibrio de clases. Este gráfico ofrece una representación más clara del mismo desequilibrio.

![StellarLabel](/assets/img/favicons/stellar/label3.png)

Por esa razón, tenemos que volver a muestrear los datos. Tenemos dos opciones: Sobremuestreo y Submuestreo. El sobremuestreo consiste en aumentar el número de muestras de la clase minoritaria, que es la clase con menos ejemplos, para equilibrar la distribución de clases. Esto suele hacerse duplicando o generando nuevas muestras para la clase minoritaria. Por otro lado, el submuestreo reduce el número de muestras en la clase mayoritaria, la clase con más ejemplos, con el mismo objetivo de lograr una distribución equilibrada de la clase. El submuestreo suele implicar la eliminación aleatoria de muestras de la clase mayoritaria.

Una de las razones por las que opté por el sobremuestreo en lugar del submuestreo es que quería conservar todos los datos. El submuestreo podría considerarse en este caso, ya que tenemos un número considerable de filas que podríamos eliminar.

En este caso vamos a utilizar SMOTE para el método de Sobremuestreo

```python
smote = SMOTE(random_state=42)

X = df.drop('class', axis=1) 
y = df['class']

X_resampled, y_resampled = smote.fit_resample(X, y)

print(y_resampled.value_counts())
```
![StellarSmote](/assets/img/favicons/stellar/smote.png)


```python
y_resampled_df = pd.DataFrame({'class': y_resampled})

plt.figure(figsize=(6, 4))
sns.countplot(x='class', data=y_resampled_df)
plt.title("Distribución de Clases después de SMOTE")
plt.xlabel("Clase")
plt.ylabel("Frecuencia")
plt.show()
```

![StellarSmoteDesp](/assets/img/favicons/stellar/smotedesp.png)

Estas son las dimensiones finales del conjunto de datos, sin valores atípicos, con las características seleccionadas y con los datos de la sobremuestra.
```python
X = X_resampled[['u', 'g', 'r', 'i', 'z', 'redshift', 'plate']]
X.shape
```

![StellarShapeFinal](/assets/img/favicons/stellar/shapeFinal.png)


Como primer paso fundamental antes de sumergirse en el terreno del aprendizaje automático, es esencial normalizar nuestro conjunto de datos. Este proceso fundamental garantiza que todas las características de los datos se escalen de forma eficaz, normalmente dentro de un intervalo común de 0 a 1. El motivo de esta práctica es crear unas condiciones equitativas para los modelos de aprendizaje automático, evitando que una sola característica influya indebidamente en los resultados. Así que, antes de embarcarnos en nuestro viaje de modelado, asegurémonos de que nuestros datos están en la misma página.


```python
df=X.copy()

scaler=MinMaxScaler()
for i in ['u', 'g', 'r', 'i', 'z', 'redshift', 'plate']:
    df[i]=scaler.fit_transform(df[[i]])

df.head()   
```

![StellarNoraml](/assets/img/favicons/stellar/normal.png)

## Modelo de Predicción

En esta sección nos centraremos en la predicción de modelos. Trabajaremos con seis modelos diferentes, cada uno de ellos elegido por sus características específicas y su adecuación a nuestro conjunto de datos y objetivos. Nuestro objetivo es examinar y evaluar a fondo el rendimiento de estos modelos para determinar cuál es el más eficaz para realizar predicciones precisas. Este proceso es un paso crucial en nuestro análisis, y lo abordaremos con precisión y objetividad.

```python
classes = ['GALAXY','STAR','QSO']

X_train, X_test, y_train, y_test = train_test_split(df, y_resampled, test_size=0.3, random_state = 42)
```

### Naïve Bayes

Para poner en marcha nuestro análisis, iniciamos el proceso con Naive Bayes como primer algoritmo elegido. Naive Bayes proporciona una evaluación inicial de nuestra exploración y predicción de datos, sentando las bases para una evaluación exhaustiva de nuestras técnicas de modelado.


```python
from sklearn.naive_bayes import GaussianNB
modelNB = GaussianNB()
modelNB.fit(X_train, y_train)
y_pred4 = modelNB.predict(X_test)

gnb_score = recall_score(y_test, y_pred4, average = 'weighted')
gnb_score
```
![StellarNaiveBayes](/assets/img/favicons/stellar/naivebayes.png)


```python
NB_cm = ConfusionMatrix(modelNB, classes=classes, cmap='GnBu')

NB_cm.fit(X_train, y_train)
NB_cm.score(X_test, y_test)
NB_cm.show()
```

![StellarNaiveBayesConsusion](/assets/img/favicons/stellar/naivebayesConfusion.png)


### Logisitc Regression

Tras nuestra exploración inicial con Naive Bayes, pasamos a aplicar la regresión logística como segundo algoritmo de nuestro análisis. La Regresión Logística ofrece una perspectiva y un enfoque diferentes a nuestro modelado predictivo, mejorando la exhaustividad de nuestra evaluación y proporcionando valiosas perspectivas sobre nuestro conjunto de datos.


```python
modelLR = LogisticRegression(max_iter=1000)
modelLR.fit(X_train, y_train)
y_pred1 = modelLR.predict(X_test)

from sklearn.metrics import recall_score
LR_score = recall_score(y_test, y_pred1, average='weighted')
print(LR_score)
```

![StellarLogReg](/assets/img/favicons/stellar/LRegg.png)



```python
NB_cm = ConfusionMatrix(modelNB, classes=classes, cmap='GnBu')

NB_cm.fit(X_train, y_train)
NB_cm.score(X_test, y_test)
NB_cm.show()
```
![StellarLRegg2](/assets/img/favicons/stellar/LRegg2.png)


### KNN

En nuestro recorrido por el mundo de los modelos predictivos, nuestro próximo destino es K-Nearest Neighbors, también conocido como KNN. KNN es nuestro tercer algoritmo y ofrece un enfoque único para comprender nuestros datos y realizar predicciones. Esta transición nos permite explorar una faceta diferente de nuestro conjunto de datos y ampliar nuestros horizontes de modelado.




```python
modelknn = KNeighborsClassifier(n_neighbors = 1)
modelknn.fit(X_train, y_train)
y_pred2 = modelknn.predict(X_test)

knn_score = recall_score(y_test, y_pred2, average='weighted')
print(knn_score)
```

![StellarKnn](/assets/img/favicons/stellar/knn.png)



```python
knn_cm = ConfusionMatrix(modelknn, classes=classes, cmap='GnBu')

knn_cm.fit(X_train, y_train)
knn_cm.score(X_test, y_test)
knn_cm.show()
```

![StellarKnn2](/assets/img/favicons/stellar/knn2.png)


### Decision Tree

A medida que avanzamos en nuestro viaje por el modelado predictivo, nuestro próximo destino es el algoritmo Árbol de decisión. El Árbol de decisión introduce una estructura arborescente que simplifica la toma de decisiones complejas. Esta transición nos proporciona un enfoque estructurado para desentrañar patrones dentro de nuestro conjunto de datos, ofreciendo un camino claro e interpretable para hacer predicciones.


```python
from sklearn.tree import DecisionTreeClassifier
modelDT = DecisionTreeClassifier(random_state = 30)
modelDT.fit(X_train, y_train)
y_pred3 = modelDT.predict(X_test)

dtree_score = recall_score(y_test, y_pred3, average='weighted')
print(dtree_score)
```

![Stellardtree](/assets/img/favicons/stellar/dtree.png)



```python
DT_cm = ConfusionMatrix(modelDT, classes=classes, cmap='GnBu')

DT_cm.fit(X_train, y_train)
DT_cm.score(X_test, y_test)
DT_cm.show()
```

![Stellardtree2](/assets/img/favicons/stellar/dtree2.png)

### Random Forest

A medida que nos adentramos en el mundo de los modelos predictivos, nuestra siguiente parada es el algoritmo Random Forest. Random Forest, el quinto modelo elegido, adopta un enfoque de conjunto al unir varios árboles de decisión para ofrecer una potente combinación de solidez y precisión. Con este cambio, pretendemos aprovechar la experiencia colectiva de estos árboles, potenciando nuestras capacidades predictivas y descubriendo más información dentro de nuestro conjunto de datos.


```python
from sklearn.ensemble import RandomForestClassifier
modelRF = RandomForestClassifier(n_estimators = 19, random_state = 30)
modelRF.fit(X_train, y_train)
y_pred5 = modelRF.predict(X_test)

from sklearn.metrics import recall_score
rf_score = recall_score(y_test, y_pred5, average = 'weighted')
print(rf_score)
```

![Stellarrforest](/assets/img/favicons/stellar/rforest.png)



```python
RF_cm = ConfusionMatrix(modelRF, classes=classes, cmap='GnBu')

RF_cm.fit(X_train, y_train)
RF_cm.score(X_test, y_test)
RF_cm.show()
```


![StellarRforest2](/assets/img/favicons/stellar/rforest2.png)


### XGBoost

Siguiendo en el terreno de la predicción, vamos a centrarnos en XGBoost como sexto y último algoritmo. XGBoost es famoso por su técnica de refuerzo de gradiente, que inyecta una gran fuerza predictiva a nuestro análisis. Esto aumenta nuestras capacidades y nos permite profundizar aún más en nuestro conjunto de datos.

```python
import xgboost as xgb
modelXG = xgb.XGBClassifier(random_state = 42)
modelXG.fit(X_train, y_train)
y_pred6 = modelXG.predict(X_test)

from sklearn.metrics import recall_score
xgb_score = recall_score(y_test, y_pred6, average = 'weighted')
print(xgb_score)
```


![StellarXGB](/assets/img/favicons/stellar/xg.png)





```python
XG_cm = ConfusionMatrix(modelXG, classes=classes, cmap='GnBu')

XG_cm.fit(X_train, y_train)
XG_cm.score(X_test, y_test)
XG_cm.show()
```


![StellarXGB2](/assets/img/favicons/stellar/xg2.png)


### Resultados

A partir de nuestro exhaustivo análisis de rendimiento, que incluye porcentajes y la matriz de confusión, hemos obtenido algunas conclusiones reveladoras. En particular, XGBoost y Random Forest aparecen como los modelos con mejor rendimiento, mostrando un notable poder predictivo. Sin embargo, es igualmente destacable que incluso el "no favorito", Naïve Bayes, lograra una encomiable tasa de precisión del 92%, ¡algo impresionante!

Profundizando en los matices revelados por la matriz de confusión, se pone de evidencia una tendencia clara. Cuando el modelo predice "Estrella", su precisión es notablemente alta. En nuestro modelo con mejores resultados, el número de estrellas predichas correctamente fue de sólo 70 de 15.165, lo que arroja un índice de error sorprendentemente bajo del 0,46%. En cambio, en el caso de clases como "Galaxia", el porcentaje de error es notablemente superior (3,297%), lo que significa que la tarea de clasificación es más difícil. Estos resultados arrojan información sobre los puntos fuertes y las complejidades de nuestros modelos predictivos.


## Conclusión

En esta parte de nuestro viaje a través del análisis predictivo, hemos abarcado un terreno importante. Empezamos familiarizándonos con el conjunto de datos SDSS y su variada gama de características. Empleando el Local Outlier Factor (LOF) para la detección de valores atípicos, logramos resultados notables. A continuación, nos adentramos en un exhaustivo análisis de características, con la ayuda de matrices de correlación, para obtener información más detallada.

Al abordar el problema del desequilibrio de datos, lo mitigamos hábilmente mediante técnicas de sobremuestreo. Estas acciones cruciales no sólo perfeccionaron nuestro conjunto de datos, sino que también nos prepararon para la última fase del modelado predictivo y la evaluación de modelos.