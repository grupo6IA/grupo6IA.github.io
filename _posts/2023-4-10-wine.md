---
layout: post
title: "Vino - Predecir la clase"
date: 2023-10-4
author: Juan Martin Maidana
categories: [Clasificación]
tags: [Data Cleaning, Data Visualization, Multiclass Classification]
image:
  path: /assets/img/favicons/wine/wine2.jpg
  alt: Wine Preview
---



# Vino - Predecir la clase

En esta ocación se trata de un conjunto de datos sobre vinos donado en 1991 con el fin de probar algunas herramientas de AI, cuyo objetivo es determinar el origen de los vinos mediante un análisis químico. Con 178 instancias y 13 características, contiene los resultados de un análisis químico de vinos cultivados en la misma región de Italia pero derivados de tres viñedos diferentes. El análisis determinó las cantidades de 13 componentes presentes en cada uno de los tres tipos de vino. Originalmente, el conjunto de datos tenía unas 30 variables, pero, por alguna razón, sólo está disponible la versión de 13 dimensiones.

## Metadata dataset

- Alcohol: El contenido de alcohol en el vino.
- Malic Acid: La cantidad de ácido málico en el vino, un tipo de ácido orgánico.
- Ash: Contenido de cenizas del vino.
- Alcalinity of Ash: Alcalinidad de la ceniza del vino.
- Magnesium: La cantidad de magnesio en el vino.
- Total Phenols: El contenido total de fenoles en el vino.
- Flavanoids: La concentración de flavonoides, un tipo de pigmento vegetal, en el vino.
- Nonflavanoid Phenols: La concentración de compuestos fenólicos no flavonoides en el vino.
- Proanthocyanins: La concentración de proantocianinas, un tipo de flavonoide, en el vino.
- Color Intensity: La intensidad del color del vino.
- Hue: La tonalidad o matiz de color del vino.
- OD280/OD315 de vinos diluidos: La densidad óptica de los vinos diluidos, una medida de la densidad del color.
- Proline: La concentración de prolina, un aminoácido, en el vino.

Estos rasgos representan diversas características químicas de los vinos y pueden utilizarse para tareas de análisis y clasificación.

## Input Data

En primer lugar vamos a importar el conjunto de datos con algunas bibliotecas básicas, y ver si se cargó correctamente, y también dar una visión general de los datos con sus atributos.


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("wine.csv")

X = df.drop('Class', axis=1)  
y = df['Class'] 

df.head()

```

![WineHead](/assets/img/favicons/wine/head.png)

>  `Click` the image to watch it fullscreen.
{: .prompt-tip }


## Data Analysis

Una vez que ya hemos comprobado que todo está correcto, podemos saltar a algunas características que tiene este conjunto de datos.

```python
#Statistical Analysis:
df.describe()
```
![WineDescription](/assets/img/favicons/wine/description.png)



```python
#Datatype Information:
df.info()
```

![WineInfo](/assets/img/favicons/wine/info.png)


De estas tablas podemos extraer alguna información útil. En primer lugar, que este conjunto de datos no tiene valores faltantes, lo que facilita el trabajo. Además, podemos ver que los rangos de los diferentes atributos tienen muchas varianzas entre sí, esto podría ser un problema cuando entrenemos este conjunto porque características como "Magnesium" tendrán mucho peso en comparación con "Nonflavanoid phenols" o "Hue". Podemos solucionar este problema utilizando un tipo de transformación, para saber qué transformación, debemos analizar la distribución de los datos.


```python
features = df.drop('Class', axis=1)

fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(12, 10))
fig.subplots_adjust(hspace=0.5)

for i, col in enumerate(features.columns):
    ax = axes[i // 4, i % 4]
    features[col].hist(bins=25, color='blue', alpha=0.7, ax=ax)
    ax.set_title(col)

plt.show()
```

![WineHistograms](/assets/img/favicons/wine/histograms.png)

Observando estos histogramas podemos ver que la mayoría de las características tienen una distribución gaussiana, por lo que la mejor opción es la transformación z. Además, aquí podemos ver que es posible que existan valores atípicos, algo que vamos a analizar más adelante. 


```python
#Applying 

scaler = StandardScaler()
X_normalized = scaler.fit_transform(features)

X_normalized_df = pd.DataFrame(X_normalized, columns=features.columns)

fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(12, 10))
fig.subplots_adjust(hspace=0.5)

for i, col in enumerate(X_normalized_df.columns):
    ax = axes[i // 4, i % 4]
    X_normalized_df[col].hist(bins=25, color='blue', alpha=0.7, ax=ax)
    ax.set_title(col)

plt.show()
```

![WineHistograms2](/assets/img/favicons/wine/histograms2.png)

Ahora que los datos tienen una media de 0 y una desviación estándar de 1, ¡tendremos resultados mucho mejores!

## Outliers

Ahora es el momento de analizar si hay algún valor atípico, para ello vamos a utilizar algún boxplot para verlo gráficamente.

```python
#lets see whether our data has outliers or not:

# create box plots
fig, ax = plt.subplots(ncols=5, nrows=3, figsize=(20,10))
index = 0
ax = ax.flatten()

for col, value in X_normalized_df.items():
    sns.boxplot(y=col, data=X_normalized_df, color='b', ax=ax[index])
    index += 1
plt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)
```

![WineBoxplot](/assets/img/favicons/wine/boxplot.png)

Aquí podemos ver que hay algunos valores atípicos en: "Malid Acid", "Ash", "Alcalinity of ash", entre otros. Pero tenemos que tener cuidado a la hora de elegir un umbral para los valores atípicos, si elegimos un valor para el umbral demasiado bajo (como 1 o 1,5) podemos eliminar una gran cantidad de datos, teniendo en cuenta que sólo tenemos 178 filas, tenemos que apuntar a un umbral más alto y eliminar aquellos valores atípicos que están más lejos de la mayoría de los datos o tienen valores significativamente más altos. El umbral elegido fue 2,7 con los siguientes resultados.

```python
def detect_outliers(column, threshold=2.7):
    mean = np.mean(column)
    std_deviation = np.std(column)
    outliers = (column - mean).abs() > threshold * std_deviation
    return outliers

columns_with_outliers = []

for column in X_normalized_df.columns:
    outliers = detect_outliers(X_normalized_df[column])
    if outliers.any():
        columns_with_outliers.append(column)
        print(f"Column '{column}' has {outliers.sum()} outliers.")
```

![WineOut](/assets/img/favicons/wine/out2.png)




```python
#data normalized and without outliers
sns.pairplot(df_sin_outliers)
```

![WineTotal](/assets/img/favicons/wine/totaldesc.png)



Ahora que los datos están normalizados y se han eliminado los valores atípicos, podemos centrarnos más en la correlación de atributos.

## Correlated Attributes

Los atributos que están correlacionados podrían marcar una gran diferencia en términos de precisión y rendimiento para muchos modelos, por eso vamos a centrarnos en este tema y específicamente en esta matriz de correlación.

```python
plt.figure(figsize=(15,10))
sns.heatmap(df.corr(), annot=True, fmt='.2f', linewidths=2)
```

![WineMatrix](/assets/img/favicons/wine/matrix.png)

Tenemos muchas cosas que analizar aquí. En primer lugar, centrándonos en el atributo 'Ash', podemos ver que tiene un peso de -0,05 en la variable objetivo, debido a esto podemos eliminar este atributo ya que es casi inútil para la etiqueta y mejorará el rendimiento. 
En segundo lugar, si nos centramos en los pesos de la etiqueta, podemos ver claramente que hay 3 atributos que tienen ±0,70 o más, lo que representa gran parte de la varianza de la etiqueta. Esos son "Total phenols", "Flavanoids" y "OD280...", la cuestión con estos atributos es que están fuertemente correlacionados entre sí (±0.70), esto podría ser un problema porque quizás están describiendo la misma cosa y no se necesitan 3 atributos. Después de pensar un rato, se me ocurrieron algunas soluciones.
- Mi primera posibilidad fue intentar fusionar "Total Phenols" y "Flavanoids".

![WineFla](/assets/img/favicons/wine/fla.png)

Como podemos ver, no sólo están correlacionadas entre sí, sino que además los valores de correlación de las demás variables son muy similares. Por ejemplo, tomando "Alcohol" como ejemplo, "Total Phenols" tiene una correlación de 0,29, mientras que "Flavanoides" tiene una correlación de 0,24. Esta tendencia se da con todos los atributos.
Después de hacer algunas pruebas de fusión de estos dos (y el tercero más tarde) los resultados me muestran que no es una buena idea, ya que dio menos precisión que no hacer nada con este problema (ignorarlo). Por eso me lancé por mi segunda y tercera posibilidad.

- Usar un tipo de reducción de dimensión.

El primero que probé fue PCA, ya que he hecho algunos trabajos sobre el mismo y los resultados fueron exitosos. Utilicé el conjunto de datos sin valores atípicos y con los datos normalizados, pero ni siquiera eso fue útil, fue una mejora en comparación con mi primera posibilidad, pero no fue suficiente. Tal vez porque PCA borrado alguna información útil. (probado más adelante en el documento)

Por eso decidí utilizar LDA (Linear Discriminant Analysis), una de las principales características de LDA es que maximiza la separación entre las diferentes clases en los datos, lo que la convierte en una técnica supervisada y con objetivo de encontrar las direcciones en las que las clases son más separables. Los resultados de utilizar LDA fueron claramente mucho mejores que los de las otras posibilidades.


## Model Prediction

Ahora es el momento de probar con diferentes modelos, con PCA, con LDA y sin nada.

### Random Forest 

El primero es Random Forest, un algoritmo que combina varios árboles de decisión para mejorar la precisión y reducir el sobreajuste. Es una opción sólida para muchas aplicaciones debido a su capacidad para manejar características categóricas y numéricas, así como para tratar datos desequilibrados.


```python
clf = RandomForestClassifier(random_state=42)
y = y[df_sin_outliers.index]

# Perform cross-validation and get accuracy scores without outliers
scores = cross_val_score(clf, df_sin_outliers, y, cv=5)
mean_accuracy = scores.mean()
print("Mean Accuracy with Random Forest after Removing Outliers:", mean_accuracy)

# PCA Dimensionality Reduction
pca = PCA(n_components=2)
X_pca = pca.fit_transform(df_sin_outliers)

# Initialize Random Forest Classifier for PCA data
clfPCA = RandomForestClassifier(random_state=42)
scoresPCA = cross_val_score(clfPCA, X_pca, y, cv=5)

mean_accuracyPCA = scoresPCA.mean()
print("Mean Accuracy with Random Forest after PCA Dimensionality Reduction:", mean_accuracyPCA)

# LDA Dimensionality Reduction
lda = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda.fit_transform(df_sin_outliers, y)

# Initialize Random Forest Classifier for LDA data
clf_rf = RandomForestClassifier(random_state=42)
scores_rf_lda = cross_val_score(clf_rf, X_lda, y, cv=5)

mean_accuracy_rf_lda = scores_rf_lda.mean()
print("Mean Accuracy with Random Forest after LDA Dimensionality Reduction:", mean_accuracy_rf_lda)
```

![WineRandomF](/assets/img/favicons/wine/randomf.png)

### SVC

El siguiente es SVC (Support Vector Classification). Se trata de un algoritmo versátil de aprendizaje automático utilizado para resolver problemas de clasificación binaria. Su objetivo principal es encontrar un límite de decisión óptimo que separe eficazmente los puntos de datos que pertenecen a clases diferentes, maximizando al mismo tiempo el margen entre ellos. El SVC es conocido por su capacidad para manejar patrones de datos no lineales y se emplea ampliamente en diversos ámbitos, lo que lo convierte en una herramienta fundamental en el campo del aprendizaje automático y la ciencia de datos.

```python
# Initialize Support Vector Classifier (SVC)
clf_svc = SVC(random_state=42)
y = y[df_sin_outliers.index]

# Perform cross-validation and get accuracy scores without outliers
scores_svc = cross_val_score(clf_svc, df_sin_outliers, y, cv=5)
mean_accuracy_svc = scores_svc.mean()
print("Mean Accuracy with SVC after Removing Outliers:", mean_accuracy_svc)

# PCA Dimensionality Reduction
pca = PCA(n_components=2)
X_pca = pca.fit_transform(df_sin_outliers)

# Initialize SVC Classifier for PCA data
clf_svc_pca = SVC(random_state=42)
scores_svc_pca = cross_val_score(clf_svc_pca, X_pca, y, cv=5)

mean_accuracy_svc_pca = scores_svc_pca.mean()
print("Mean Accuracy with SVC after PCA Dimensionality Reduction:", mean_accuracy_svc_pca)

# LDA Dimensionality Reduction
lda = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda.fit_transform(df_sin_outliers, y)

# Initialize SVC Classifier for LDA data
clf_svc_lda = SVC(random_state=42)
scores_svc_lda = cross_val_score(clf_svc_lda, X_lda, y, cv=5)

mean_accuracy_svc_lda = scores_svc_lda.mean()
print("Mean Accuracy with SVC after LDA Dimensionality Reduction:", mean_accuracy_svc_lda)
```
![WineSVC](/assets/img/favicons/wine/svc.png)

### KNN

Y por último, KNN (K-Nearest Neighbors). Funciona encontrando los k puntos de datos más cercanos a uno dado y haciendo predicciones basadas en su clase mayoritaria (para clasificación) o su media (para regresión). KNN es sencillo, no asume distribuciones de datos y se aplica ampliamente en el reconocimiento de patrones y el análisis de datos.


```python
# Initialize k-Nearest Neighbors (KNN) Classifier
clf_knn = KNeighborsClassifier(n_neighbors=5)
y = y[df_sin_outliers.index]

# Perform cross-validation and get accuracy scores without outliers
scores_knn = cross_val_score(clf_knn, df_sin_outliers, y, cv=5)
mean_accuracy_knn = scores_knn.mean()
print("Mean Accuracy with KNN after Removing Outliers:", mean_accuracy_knn)

# PCA Dimensionality Reduction
pca = PCA(n_components=2)
X_pca = pca.fit_transform(df_sin_outliers)

# Initialize KNN Classifier for PCA data
clf_knn_pca = KNeighborsClassifier(n_neighbors=5)
scores_knn_pca = cross_val_score(clf_knn_pca, X_pca, y, cv=5)

mean_accuracy_knn_pca = scores_knn_pca.mean()
print("Mean Accuracy with KNN after PCA Dimensionality Reduction:", mean_accuracy_knn_pca)

# LDA Dimensionality Reduction
lda = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda.fit_transform(df_sin_outliers, y)

# Initialize KNN Classifier for LDA data
clf_knn_lda = KNeighborsClassifier(n_neighbors=5)
scores_knn_lda = cross_val_score(clf_knn_lda, X_lda, y, cv=5)

mean_accuracy_knn_lda = scores_knn_lda.mean()
print("Mean Accuracy with KNN after LDA Dimensionality Reduction:", mean_accuracy_knn_lda)
```

![WineKNN](/assets/img/favicons/wine/knn.png)

## Conclusión

Este documento presenta un análisis exhaustivo del conjunto de datos Wine, comenzando con un examen minucioso de los datos y abordando las complejidades de los atributos correlacionados. A lo largo de nuestra exploración, hemos evaluado varias estrategias y, en última instancia, hemos llegado a la conclusión de que KNN y SVC son los modelos con mejores resultados. Sin embargo, debemos abordar estos resultados con cautela. Cabe destacar que LDA no falló en ninguna predicción, especialmente en modelos como SVC. Esto podría indicar que el modelo está sobreajustado. Aunque podría ser aceptable en este caso debido al tamaño y las características del conjunto de datos, en el futuro, para mantener el rendimiento del modelo, podría ser necesario un reentrenamiento para garantizar resultados más precisos y exactos. Sin embargo, hemos observado que la reducción de la dimensionalidad mediante LDA ha dado mejores resultados que otras técnicas alternativas.
