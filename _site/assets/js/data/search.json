[
  
  {
    "title": "CKD",
    "url": "/grupo6IA.github.io/posts/ckd/",
    "categories": "Clasificación",
    "tags": "Data Cleaning, Naive Bayes, KNN, Logistic Regression, Rapid Miner",
    "date": "2023-10-24 00:00:00 -0300",
    





    
    "snippet": "CKDAproximadamente el 10% de la población mundial tiene algún grado de enfermedad renal; Lamentablemente pasa desapercibido porque sus síntomas aparecen muy tarde. Sus principales factores de riesg...",
    "content": "CKDAproximadamente el 10% de la población mundial tiene algún grado de enfermedad renal; Lamentablemente pasa desapercibido porque sus síntomas aparecen muy tarde. Sus principales factores de riesgo para desarrollarla, como hipertensión arterial, diabetes, obesidad, antecedentes familiares de enfermedad renal y edad mayor de 60 años, son muy prevalentes a nivel mundial.Debido a su alto costo, las terapias de reemplazo de la insuficiencia renal, como la diálisis y el trasplante de riñón, son inaccesibles para la mayoría de la población, especialmente en los países en desarrollo. De ahí la importancia de detectar oportunamente la enfermedad renal y establecer medidas terapéuticas para frenar su progresión.La enfermedad renal se puede prevenir si se detecta a tiempo. Una prueba de creatinina en sangre y una prueba de proteínas en orina son suficientes para detectar su presencia. La aplicación de medidas como un buen control de la presión arterial, la glucemia, el mantenimiento de un peso ideal, así como el uso de medicamentos que protejan la función renal, pueden frenar el deterioro de la función renal y así prevenir o retrasar la necesidad de diálisis y trasplante. .Para lograr lo anterior, es necesario concientizar y educar a la población en general, incluidos los profesionales de la salud, sobre las medidas necesarias para detectar y tratar oportunamente la enfermedad renal.El objetivo de este conjunto de datos es predecir si una persona tiene o no una enfermedad renal crítica (CKD).Metadata datasetMissing ValuesAlgo a tener en cuenta en este conjunto de datos es la gran cantidad de atributos que faltan. Para esto, se utiliza un operador “Declarar valor faltante” para marcar el “?” como valores faltantes y juegos, reemplácelos con el promedio usando el operador “Reemplazar valores faltantes”. De hecho, sería mejor utilizar otra forma más apropiada de reemplazar los valores faltantes, por ejemplo una en la que la distribución de los datos no se vea afectada.Analysis of ExamplesDebido al desconocimiento sobre el tema y a la imposibilidad de crear una matriz de correlación, no es posible afirmar si un atributo tiene más relevancia que otro en la variable objetivo. Sería apropiado contar con expertos en el tema que avalen las decisiones.OutliersPasando a los valores atípicos, no hay mucha información sobre cómo estos afectan la variable objetivo; por ejemplo, una muestra con un valor demasiado alto de “recuento de glóbulos blancos” podría tomarse como un valor atípico. Sin embargo, a priori no eliminaremos dicho valor atípico ya que este caso podría ser significativo para la presencia de ERC. Entonces, como primera instancia de modelado, no se realizará ningún tratamiento de valores atípicos; en caso de que el desempeño no sea el adecuado, se considerará algún tipo de tratamiento.BiasesLos únicos casos en los que se necesita una distribución normal son los de valores numéricos al modelar con el algoritmo Naive-Bayes. Hay algunos casos de atributos con distribuciones ligeramente “extrañas”, por ejemplo la creatinina sérica, en los que la gran mayoría de las muestras se limitan a un rango determinado excepto un par que podrían considerarse anomalías. Pese a ello, no se realizarán normalizaciones para no afectar el impacto de estos valores en las predicciones.ModelingDado que estamos ante un problema de clasificación en el que la variable objetivo es binomial (cdk o no cdk), intentaremos ejecutar el modelo con tres algoritmos.La primera tarea a realizar dentro de Rapid Miner será importar el conjunto de datos y marcar nuestra variable de destino utilizando el operador Establecer rol. Después de eso, se aplica el operador Declarar valor faltante para declarar algunos campos como valores faltantes. A continuación, se aplica el operador Reemplazar valores faltantes para reemplazar los valores faltantes con el promedio de cada atributo. Después de eso, se utiliza un operador Eliminar valores no utilizados para eliminar la posibilidad de que algunos atributos tengan un “?” valor.Una vez preparado correctamente el conjunto de datos, se comenzarán a aplicar los diferentes algoritmos de clasificación para evaluar cuál da mejores resultados. Para esto se utiliza el operador Multiplicar para multiplicar el conjunto de datos por 3 y así poder aplicar Regresión Logística, Naive Bayes y KNN. Para aplicar cada uno de ellos, se utiliza el operador de Validación Cruzada para dividir nuestro conjunto de datos en una parte de entrenamiento y una parte de prueba.El proceso de Rapid Miner debería verse así:Dentro de cada Cross Validation existe el algoritmo a utilizar y el operador Performance, para evaluar el desempeño de cada uno de ellos. A continuación puedes ver la configuración de cada Cross Validation y el resultado de cada algoritmo.Logistic RegressionNaive BayesKNNResultsComo se puede observar, todos los algoritmos obtuvieron un resultado bastante alto (cercano al 100%), por lo que en un contexto donde el funcionamiento del modelo debe ser lo más perfecto posible ya que estamos tratando de determinar si una persona tiene o no enfermedad renal, este porcentaje puede ser aceptado."
  },
  
  {
    "title": "Census",
    "url": "/grupo6IA.github.io/posts/census/",
    "categories": "Clasificación",
    "tags": "Data Cleaning, Naive Bayes, Rapid Miner, Supervised",
    "date": "2023-10-24 00:00:00 -0300",
    





    
    "snippet": "CensusEl conjunto de datos del censo de Estados Unidos de 1994, compilado por Barry Becker, es una valiosa fuente de información para analizar la demografía y la economía de ese período en el país....",
    "content": "CensusEl conjunto de datos del censo de Estados Unidos de 1994, compilado por Barry Becker, es una valiosa fuente de información para analizar la demografía y la economía de ese período en el país. Este conjunto de datos incluye una amplia variedad de características individuales, como edad, sexo, nivel educativo, ocupación, estado civil y más. El objetivo de este conjunto de datos es predecir si una persona ganó o no más de $50,000 en 1994, con base en algunos atributos de la persona como: su nivel de educación, su trabajo, edad, entre otros.Metadata datasetEste conjunto de datos tiene 14 variables predictoras, más 1 variable objetivo, que es la que desea predecir. De estas 14 variables, 6 son numéricas y el resto categóricas (incluida la variable objetivo). Las variables contenidas en el conjunto de datos son las siguientes:  Age: Edad de la persona  Workclass: Representa el estado laboral del individuo.  Fnlwgt: Peso final. En otras palabras, este es el número de personas que el censo cree que representa la entrada.  Education: Máxima educación alcanzada por la persona.  Education-num: El nivel más alto de educación alcanzado en forma numérica.  Marital-status: Representa lo que es este individuo en relación con los demás.  Occupation: El tipo general de ocupación de un individuo.  Relationship: Representa lo que este individuo es en relación con los demás.  Race: Descripción de la raza de un individuo.  Sex: El sexo biológico del individuo.  Capital-gain: Ganancias patrimoniales para una persona física.  Capital-loss: Pérdida de capital para un individuo.  Hours-per-week: Las horas que una persona ha reportado trabajar por semana.  Native-country: País de origen de un individuo.  Income: Es la variable objetivo si ganó o no 50k dólares o más en todo el año. (&lt;=50K, &gt;50K)Data AnalysisEn este caso de estudio, utilicé el conjunto de datos de Ingresos del Censo que está disponible en el repositorio de la UCI. Tiene 32000 filas de datos. Puedes encontrar el conjunto de datos en este enalce.Missing ValuesHay 2399 filas que contienen datos faltantes, en los atributos de ocupación, clase laboral y país de origen. En primera instancia se decidió excluir estas filas para realizar una primera evaluación y luego comparar reemplazando los valores faltantes por el más común en el conjunto de datos, para ver cuál es más efectivo.Analysis of ExamplesDe este conjunto de variables se puede elegir si descartar educación o educación-num, ya que ambas representan lo mismo, el máximo estudio alcanzado por una persona. En este caso descarté la educación, ya que education-num nos permite saber rápidamente si un individuo tiene un mayor nivel de educación que otro, debido al orden numérico que se genera.CorrelationAl observar la matriz de correlación, podemos ver que el ingreso y el fnlwgt tienen una correlación muy baja, por lo que podemos excluir esta variable del modelo. Por otro lado, vemos que la educación y los ingresos están bastante correlacionados, por lo que podemos suponer que tendrá un gran impacto en lo que respecta a la predicción.OutliersHay algunos casos de valores atípicos, particularmente en el atributo “ganancia de capital”, donde puedes encontrar 40 casos en los que tu beneficio está entre 90.000 y 100.000 dólares, mientras que en la mayoría de los casos están entre 0 y 20.000 dólares. Estos 40 casos fueron descartados para el análisis de este conjunto de datos.BiasesViendo las gráficas de algunos de los atributos podemos notar que la gran mayoría de los entrevistados en el censo eran trabajadores privados (6816).Y en el caso de la educación, todos los datos se concentran en los valores del 8 al 12, que se refiere a personas que han completado la secundaria.ModelingA la hora de decidir qué modelos desarrollar o algoritmos utilizar, es fundamental considerar el tipo de problema, que en este caso consiste en predecir si el salario de una persona es mayor o no a 50K dólares, a partir de un conjunto de predictores y por tanto es supervisado y de clasificación. Existe un conjunto de algoritmos que se pueden utilizar en estas condiciones; en este caso decidí utilizar el enfoque Naive Bayes.La primera tarea a realizar dentro de Rapid Miner será importar el conjunto de datos y marcar nuestra variable de destino utilizando el operador Establecer rol. Después de eso, se aplica el operador Filtrar ejemplos para eliminar los atributos que eran iguales a “?”. A continuación, se aplica el operador Seleccionar atributos para eliminar los atributos de educación y fnlwgt del conjunto de datos.Una vez que el conjunto de datos se haya preparado correctamente, se comenzará a aplicar y evaluar Naive Bayes para ver el rendimiento. Para ello, se utiliza el operador Split Data para dividir el conjunto de datos en dos partes, una para entrenamiento y otra para pruebas. Una vez que se divide el conjunto de datos, podemos aplicar Naive Bayes y usar el operador de rendimiento para evaluar qué tan bien predijo nuestro modelo.El proceso de Rapid Miner debería verse así:ResultsComo puede verse, el modelo tuvo una precisión del 82,26%, lo que es un muy buen resultado para el contexto del conjunto de datos. Podría ser mejor, tal vez usar la validación cruzada y ver si cambia el resultado. Pruebe también si otros algoritmos habrían funcionado mejor que Naive Bayes. Pero como mencioné antes, 82,26% es un muy buen resultado para predecir si una persona ganó o no 50.000 dólares en todo el año."
  },
  
  {
    "title": "Clasificación Estelar - SDSS17",
    "url": "/grupo6IA.github.io/posts/stellar/",
    "categories": "Clasificación",
    "tags": "Remuestreo, Visualización de Datos, Clasificación Multiclase",
    "date": "2023-10-09 00:00:00 -0300",
    





    
    "snippet": "Stellar Classification - SDSS17Uno de los temas que siempre han fascinado es el espacio. La inmensidad del universo, los misterios de las galaxias y la belleza del cielo nocturno siempre han manten...",
    "content": "Stellar Classification - SDSS17Uno de los temas que siempre han fascinado es el espacio. La inmensidad del universo, los misterios de las galaxias y la belleza del cielo nocturno siempre han mantenido la curiosidad de la humanidad. En el mundo de la astronomía, una de las tareas fundamentales es la clasificación de los objetos celestes, y el “Stellar Classification Dataset - SDSS17” ofrece una oportunidad apasionante para explorar y comprender las características espectrales de estrellas, galaxias y cuásares. Este conjunto de datos, basado en observaciones del Sloan Digital Sky Survey, contiene mucha información que nos permite clasificar estos objetos y adentrarnos en las maravillas de nuestro cosmos. Embarquémonos en este viaje astronómico y analicemos este conjunto de datos.Metadata DatasetContexto - SDSS17El Sloan Digital Sky Survey (SDSS), a  frecuentemente denominado simplemente SDSS, constituye un logro monumental en el campo de la astronomía, pionero en desvelar los secretos de nuestro vasto cosmos. Este estudio multiespectral de imágenes y corrimientos al rojo espectroscópicos se lleva a cabo con un telescopio óptico gran angular de 2,5 metros situado en el Observatorio de Apache Point, en Nuevo México (Estados Unidos). Desde su creación en el año 2000, el SDSS ha transformado radicalmente nuestra comprensión del universo.En esencia, el SDSS se basó en una instrumentación y unas técnicas de procesamiento de datos revolucionarias. Utilizaba una cámara CCD de barrido con múltiples filtros y matrices para capturar imágenes de alta calidad del cielo nocturno. Estas imágenes se complementaban con un espectrógrafo multiobjeto/multifibra capaz de obtener simultáneamente espectros de numerosos objetos celestes.Uno de los principales retos del proyecto fue hacer frente a la gran catidad de volumen de datos generados cada noche por el telescopio y los instrumentos. Con el tiempo, estos datos se procesaron y dieron lugar a la creación de extensos catálogos de objetos astronómicos, disponibles para su consulta digital. El SDSS desempeñó un papel fundamental en el avance de las tecnologías de almacenamiento y accesibilidad de bases de datos masivas, haciendo que sus datos fueran ampliamente accesibles a través de Internet, un concepto relativamente novedoso en aquella época.La cámara de imágenes recoge datos de imágenes fotométricas utilizando un conjunto de 30 CCD SITe/Tektronix de 2048 por 2048 píxeles colocados en seis columnas de cinco CCD cada una, alineadas con las columnas de píxeles de los propios CCD. Los filtros r, i, u, z y g del SDSS cubren las filas respectivas del conjunto, en ese orden. El sondeo opera el instrumento en modo de barrido a la deriva: la cámara lee lentamente los CCDs a medida que se recogen los datos, mientras el telescopio se mueve a lo largo de grandes círculos en el cielo, de modo que las imágenes de los objetos se mueven a lo largo de las columnas de los CCDs a la misma velocidad a la que se leen los CCDs.Contenido - SDSS17El conjunto de datos comprende 100.000 observaciones de objetos celestes captadas por el Sloan Digital Sky Survey (SDSS). Cada observación se caracteriza por 17 columnas de características y 1 columna de clase, que clasifica el objeto como estrella, galaxia o cuásar.He aquí una descripción detallada de las columnas del conjunto de datos:      obj_ID (Identificador de objeto): Se trata de un identificador único asignado a cada objeto en el catálogo de imágenes utilizado por el CAS (Catalog Archive Server).        alpha (Ángulo de ascensión recta): El ángulo de ascensión recta, medido en la época J2000, especifica la posición este-oeste del objeto en el cielo.        delta (ángulo de declinación): El ángulo de declinación, medido en la época J2000, especifica la posición norte-sur del objeto en el cielo.        u (Filtro ultravioleta): El filtro u corresponde a la porción ultravioleta del sistema fotométrico y proporciona información sobre el brillo del objeto en este rango.        g (Filtro verde): El filtro g corresponde a la parte verde del sistema fotométrico y mide el brillo del objeto en la longitud de onda verde.        r (Filtro rojo): El filtro r corresponde a la parte roja del sistema fotométrico y mide el brillo del objeto en la longitud de onda del rojo.        i (Filtro infrarrojo cercano): El filtro i corresponde a la porción infrarroja cercana del sistema fotométrico y mide el brillo del objeto en este rango.        z (Filtro infrarrojo): El filtro z corresponde a la porción infrarroja del sistema fotométrico y proporciona información sobre el brillo del objeto en la longitud de onda infrarroja.        run_ID (Número de ejecución): Se trata de un identificador único utilizado para especificar la exploración concreta durante la cual se realizó la observación.        rereun_ID (Número de repetición): El número de repetición especifica cómo se procesó la imagen, indicando cualquier reprocesamiento o modificación aplicada a los datos.        cam_col (Columna de cámara): El número de columna de la cámara identifica la línea de barrido específica dentro de la ejecución.        field_ID (Número de campo): El número de campo se utiliza para identificar cada campo, que es una región específica del cielo.        spec_obj_ID (Identificador de objeto espectroscópico): Se trata de un identificador único para objetos espectroscópicos ópticos. Las observaciones con el mismo spec_obj_ID comparten la misma clase de salida.        class (Clase de objeto): Esta columna categoriza el tipo de objeto celeste observado, incluyendo galaxias, estrellas o cuásares.        redshift: El valor del desplazamiento al rojo se basa en el aumento de la longitud de onda y es una medida fundamental para comprender el movimiento y la distancia de los objetos en el universo.        plate (Plate ID): El ID de placa identifica cada placa utilizada en el SDSS. Las placas son objetos físicos con agujeros perforados para permitir que las fibras recojan la luz de los objetos celestes.        MJD (Fecha Juliana Modificada): La MJD se utiliza para indicar cuándo se recogió un dato concreto del SDSS, proporcionando una marca de tiempo precisa para cada observación.        fiber_ID (ID de fibra): El ID de fibra identifica la fibra específica utilizada para apuntar la luz al plano focal durante cada observación. Esto permite al SDSS recoger luz de diferentes objetos simultáneamente.  Input dataEl primer paso a realizar es importar nuestras librerías que utilizaremos a lo largo del documento, así como comprobar si se han cargado correctamente imprimiendo las primeras líneas del conjunto de datos.import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.neighbors import LocalOutlierFactorfrom sklearn.preprocessing import LabelEncoderfrom imblearn.over_sampling import SMOTEfrom sklearn.linear_model import LogisticRegressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.preprocessing import MinMaxScalerfrom yellowbrick.classifier import ConfusionMatrixfrom lightgbm import LGBMClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import StratifiedKFoldfold = StratifiedKFold(n_splits=4, shuffle = True, random_state=42)from sklearn.metrics import recall_scorefrom warnings import filterwarningsfilterwarnings(action='ignore')df = pd.read_csv(\"star_classification.csv\")df.head()Aquí encontrará información adicional sobre este conjunto de datos, filas, información estadística, entre otros.df.shapeAjustando la variable objetivodf.info()La observación al ver estos datos es que la etiqueta es de tipo objeto, algo que cambiamos a numérico para poder utilizarlo en modelos en el futuro.df[\"class\"]=[0 if i == \"GALAXY\" else 1 if i == \"STAR\" else 2 for i in df[\"class\"]]Con este código cambiaremos “GALAXY” por el número 0, “STAR” por el número 1, y “Quasars” por el número 2. También podemos ver en la tabla de información que en 100.000 filas no falta ningún valor, una gran noticia ya que nos facilitará el trabajo de preparacion de datos.OutliersCuando nos enfrentamos a los valores atípicos en un conjunto de datos tan grande, tenemos que ser más cuidadosos, por eso se decido utilizar un enfoque más robusto, éste algorimo fue el Local Outlier Factor (LOF). La idea del LOF es comparar cada punto de datos con sus vecinos cercanos. Calcula a qué distancia está cada punto de sus vecinos y comprueba si un punto está mucho más lejos de sus vecinos de lo que están entre sí. Si está mucho más lejos, se considera una anomalía. Si no está mucho más lejos, se considera un punto normal. LOF utiliza esta comparación de densidad relativa para encontrar anomalías locales en un conjunto de datos.clf = LocalOutlierFactor()y_pred = clf.fit_predict(df)x_score = clf.negative_outlier_factor_outlier_score = pd.DataFrame()outlier_score[\"score\"] = x_score#thresholdthreshold2 = -1.5filtre2 = outlier_score[\"score\"] &lt; threshold2outlier_index = outlier_score[filtre2].index.tolist()len(outlier_index)df.drop(outlier_index, inplace=True)df.shapeDespués de eliminar los valores atípicos con LOF obtenemos un total de 84744 filas en 18 características, ahora es el momento de echar un vistazo más de cerca a los atributos.Visualización de DataMirando a la variable objetivo podemos ver que los datos están realmente desequilibrados, casi el 60% de las filas son galaxias, y el 19% son Cuásares. Este es un problema que vamos a atacar en el futuro, en el momento de entrenar un modelo, pero por ahora vamos a echar un vistazo a las otras características.Una vez que analizamos un poco la etiqueta, ahora podemos centrarnos en las características, en este gráfico analizamos la distribución y densidad de cada característica en relación con la etiqueta objetivo. Aunque este gráfico es exhaustivo, proporciona información valiosa. En el lado izquierdo, puede ver las densidades de las características representadas por los gráficos de estimación de densidad de Kernel (KDE). Estos gráficos muestran cómo varía la distribución de cada característica en las tres clases y proporcionan una visión agregada de todas las clases. A la derecha, los histogramas ilustran las mismas distribuciones. Esta visualización es muy útil para evaluar la importancia de las características y las relaciones lineales con la etiqueta.Feature Selection# Select the features you want to visualize in the plotsfeatures = df.drop('class', axis=1)# Mapping of real labels to desired labelslabel_mapping = {0: 'Galaxy', 1: 'Star', 2: 'Quasar'}# Create a grid of subplotsfig, axes = plt.subplots(nrows=len(features.columns), ncols=2, figsize=(12, 2*len(features.columns)))fig.subplots_adjust(hspace=0.5)# Iterate through the features and generate plotsfor i, col in enumerate(features.columns):    # KDE Plot    ax1 = axes[i, 0]    for label_value, label_name in label_mapping.items():        sns.kdeplot(data=df[df[\"class\"] == label_value][col], label=label_name, ax=ax1)    sns.kdeplot(data=df[col], label=\"All\", ax=ax1)    ax1.set_title(f'{col} KDE')    ax1.legend()    # Histogram Plot    ax2 = axes[i, 1]    for label_value, label_name in label_mapping.items():        sns.histplot(data=df[df[\"class\"] == label_value][col], bins=25, color='blue', alpha=0.7, ax=ax2, label=label_name)    sns.histplot(data=df[col], bins=25, color='red', alpha=0.5, ax=ax2, label=\"All\")    ax2.set_title(f'{col} Histogram')    ax2.legend()plt.tight_layout()plt.show()Centrándonos más en los gráficos de densidad, tenemos que analizar la relación lineal con la etiqueta, por ejemplo, en “obj_id” podemos ver que para cualquier valor de “obj_id” la etiqueta no tiene ninguna varianza. La densidad en diferentes valores de “obj_id” es siempre la misma para las 3 clases (las 3 clases siguen la misma tendencia), de esto podemos deducir que esta característica podría ser  inútil para la varianza de la etiqueta.Podemos ver algo similar para “alpha” y “beta”, sabemos que estos atributos definen la ubicación donde se encuentra el objeto celeste en el espacio, y con estos gráficos podemos decir que la ubicación del objeto celeste no es una característica importante para definir si se trata de una galaxia, estrella o cuásar.Hay algunas características que siguen esta tendencia (“run_Id”, “cam_col”, “field_Id”, entre otras), algunas de ellas porque son sólo identificadores o características no útiles para el modelo pero importantes para algunos astrólogos.Pero algunas características que podemos ver claramente que pueden ser importantes para la etiqueta, por ejemplo: “u”, “g”, “r”, “i”, “z” y “plate”. Por ejemplo, en el caso de “plate” podemos ver muchas variaciones a lo largo de los valores de “plate”, desde valores de 2000 a 4000 podemos ver que tenemos altas probabilidades de que sea una estrella, y de 7000 a 9500 podría ser un cuásar. Esto se puede ver en las características mencionadas anteriormente.Observando la matriz de correlaciones obtenemos los siguientes resultados.Como se ha dicho, algunas de las características que ya hemos mencionado son las que están más correlacionadas, pero “redshift” tiene la correlación lineal más alta entre todas las características, esto significa, que tiene mucho peso en la decisión de la etiqueta con un 0,54, echando un vistazo más de cerca a esta característica obtenemos el siguiente gráfico.  0 para Galaxia, 1 for Estrella, 2 for Quasar.Ahora que miramos más de cerca, podemos ver claramente la diferencia en densidad para diferentes valores del “redshift”, de -15 a -5 podemos ver que la densidad se dispara para Estrella, mientras que de -3 a -0.5 parece ser una Galaxia, y para valores superiores a 1, es probable que sea un Cuásar.En este otro gráfico se puede ver fácilmente la separación entre el valor de desplazamiento al rojo y las clases.Basándonos en estos gráficos podemos concluir que existe una fuerte correlación lineal entre el redshift y la etiquetaA partir de estos gráficos se concluye que características vamos a utilizar en los modelos, nos deshacemos de otras ya que hay ids u otros atributos que no tienen casi correlación con la variable.Datos DesequilibradosComo observamos en el gráfico anterior, que representa las etiquetas, es evidente que nuestro conjunto de datos sufre un desequilibrio de clases. Este gráfico ofrece una representación más clara del mismo desequilibrio.Por esa razón, tenemos que volver a muestrear los datos. Tenemos dos opciones: Sobremuestreo y Submuestreo. El sobremuestreo consiste en aumentar el número de muestras de la clase minoritaria, que es la clase con menos ejemplos, para equilibrar la distribución de clases. Esto suele hacerse duplicando o generando nuevas muestras para la clase minoritaria. Por otro lado, el submuestreo reduce el número de muestras en la clase mayoritaria, la clase con más ejemplos, con el mismo objetivo de lograr una distribución equilibrada de la clase. El submuestreo suele implicar la eliminación aleatoria de muestras de la clase mayoritaria.Una de las razones por las que opté por el sobremuestreo en lugar del submuestreo es que quería conservar todos los datos. El submuestreo podría considerarse en este caso, ya que tenemos un número considerable de filas que podríamos eliminar.En este caso vamos a utilizar SMOTE para el método de Sobremuestreosmote = SMOTE(random_state=42)X = df.drop('class', axis=1) y = df['class']X_resampled, y_resampled = smote.fit_resample(X, y)print(y_resampled.value_counts())y_resampled_df = pd.DataFrame({'class': y_resampled})plt.figure(figsize=(6, 4))sns.countplot(x='class', data=y_resampled_df)plt.title(\"Distribución de Clases después de SMOTE\")plt.xlabel(\"Clase\")plt.ylabel(\"Frecuencia\")plt.show()Estas son las dimensiones finales del conjunto de datos, sin valores atípicos, con las características seleccionadas y con los datos de la sobremuestra.X = X_resampled[['u', 'g', 'r', 'i', 'z', 'redshift', 'plate']]X.shapeComo primer paso fundamental antes de sumergirse en el terreno del aprendizaje automático, es esencial normalizar nuestro conjunto de datos. Este proceso fundamental garantiza que todas las características de los datos se escalen de forma eficaz, normalmente dentro de un intervalo común de 0 a 1. El motivo de esta práctica es crear unas condiciones equitativas para los modelos de aprendizaje automático, evitando que una sola característica influya indebidamente en los resultados. Así que, antes de embarcarnos en nuestro viaje de modelado, asegurémonos de que nuestros datos están en la misma página.df=X.copy()scaler=MinMaxScaler()for i in ['u', 'g', 'r', 'i', 'z', 'redshift', 'plate']:    df[i]=scaler.fit_transform(df[[i]])df.head()   Modelo de PredicciónEn esta sección nos centraremos en la predicción de modelos. Trabajaremos con seis modelos diferentes, cada uno de ellos elegido por sus características específicas y su adecuación a nuestro conjunto de datos y objetivos. Nuestro objetivo es examinar y evaluar a fondo el rendimiento de estos modelos para determinar cuál es el más eficaz para realizar predicciones precisas. Este proceso es un paso crucial en nuestro análisis, y lo abordaremos con precisión y objetividad.classes = ['GALAXY','STAR','QSO']X_train, X_test, y_train, y_test = train_test_split(df, y_resampled, test_size=0.3, random_state = 42)Naïve BayesPara poner en marcha nuestro análisis, iniciamos el proceso con Naive Bayes como primer algoritmo elegido. Naive Bayes proporciona una evaluación inicial de nuestra exploración y predicción de datos, sentando las bases para una evaluación exhaustiva de nuestras técnicas de modelado.from sklearn.naive_bayes import GaussianNBmodelNB = GaussianNB()modelNB.fit(X_train, y_train)y_pred4 = modelNB.predict(X_test)gnb_score = recall_score(y_test, y_pred4, average = 'weighted')gnb_scoreNB_cm = ConfusionMatrix(modelNB, classes=classes, cmap='GnBu')NB_cm.fit(X_train, y_train)NB_cm.score(X_test, y_test)NB_cm.show()Logisitc RegressionTras nuestra exploración inicial con Naive Bayes, pasamos a aplicar la regresión logística como segundo algoritmo de nuestro análisis. La Regresión Logística ofrece una perspectiva y un enfoque diferentes a nuestro modelado predictivo, mejorando la exhaustividad de nuestra evaluación y proporcionando valiosas perspectivas sobre nuestro conjunto de datos.modelLR = LogisticRegression(max_iter=1000)modelLR.fit(X_train, y_train)y_pred1 = modelLR.predict(X_test)from sklearn.metrics import recall_scoreLR_score = recall_score(y_test, y_pred1, average='weighted')print(LR_score)NB_cm = ConfusionMatrix(modelNB, classes=classes, cmap='GnBu')NB_cm.fit(X_train, y_train)NB_cm.score(X_test, y_test)NB_cm.show()KNNEn nuestro recorrido por el mundo de los modelos predictivos, nuestro próximo destino es K-Nearest Neighbors, también conocido como KNN. KNN es nuestro tercer algoritmo y ofrece un enfoque único para comprender nuestros datos y realizar predicciones. Esta transición nos permite explorar una faceta diferente de nuestro conjunto de datos y ampliar nuestros horizontes de modelado.modelknn = KNeighborsClassifier(n_neighbors = 1)modelknn.fit(X_train, y_train)y_pred2 = modelknn.predict(X_test)knn_score = recall_score(y_test, y_pred2, average='weighted')print(knn_score)knn_cm = ConfusionMatrix(modelknn, classes=classes, cmap='GnBu')knn_cm.fit(X_train, y_train)knn_cm.score(X_test, y_test)knn_cm.show()Decision TreeA medida que avanzamos en nuestro viaje por el modelado predictivo, nuestro próximo destino es el algoritmo Árbol de decisión. El Árbol de decisión introduce una estructura arborescente que simplifica la toma de decisiones complejas. Esta transición nos proporciona un enfoque estructurado para desentrañar patrones dentro de nuestro conjunto de datos, ofreciendo un camino claro e interpretable para hacer predicciones.from sklearn.tree import DecisionTreeClassifiermodelDT = DecisionTreeClassifier(random_state = 30)modelDT.fit(X_train, y_train)y_pred3 = modelDT.predict(X_test)dtree_score = recall_score(y_test, y_pred3, average='weighted')print(dtree_score)DT_cm = ConfusionMatrix(modelDT, classes=classes, cmap='GnBu')DT_cm.fit(X_train, y_train)DT_cm.score(X_test, y_test)DT_cm.show()Random ForestA medida que nos adentramos en el mundo de los modelos predictivos, nuestra siguiente parada es el algoritmo Random Forest. Random Forest, el quinto modelo elegido, adopta un enfoque de conjunto al unir varios árboles de decisión para ofrecer una potente combinación de solidez y precisión. Con este cambio, pretendemos aprovechar la experiencia colectiva de estos árboles, potenciando nuestras capacidades predictivas y descubriendo más información dentro de nuestro conjunto de datos.from sklearn.ensemble import RandomForestClassifiermodelRF = RandomForestClassifier(n_estimators = 19, random_state = 30)modelRF.fit(X_train, y_train)y_pred5 = modelRF.predict(X_test)from sklearn.metrics import recall_scorerf_score = recall_score(y_test, y_pred5, average = 'weighted')print(rf_score)RF_cm = ConfusionMatrix(modelRF, classes=classes, cmap='GnBu')RF_cm.fit(X_train, y_train)RF_cm.score(X_test, y_test)RF_cm.show()XGBoostSiguiendo en el terreno de la predicción, vamos a centrarnos en XGBoost como sexto y último algoritmo. XGBoost es famoso por su técnica de refuerzo de gradiente, que inyecta una gran fuerza predictiva a nuestro análisis. Esto aumenta nuestras capacidades y nos permite profundizar aún más en nuestro conjunto de datos.import xgboost as xgbmodelXG = xgb.XGBClassifier(random_state = 42)modelXG.fit(X_train, y_train)y_pred6 = modelXG.predict(X_test)from sklearn.metrics import recall_scorexgb_score = recall_score(y_test, y_pred6, average = 'weighted')print(xgb_score)XG_cm = ConfusionMatrix(modelXG, classes=classes, cmap='GnBu')XG_cm.fit(X_train, y_train)XG_cm.score(X_test, y_test)XG_cm.show()ResultadosA partir de nuestro exhaustivo análisis de rendimiento, que incluye porcentajes y la matriz de confusión, hemos obtenido algunas conclusiones reveladoras. En particular, XGBoost y Random Forest aparecen como los modelos con mejor rendimiento, mostrando un notable poder predictivo. Sin embargo, es igualmente destacable que incluso el “no favorito”, Naïve Bayes, lograra una encomiable tasa de precisión del 92%, ¡algo impresionante!Profundizando en los matices revelados por la matriz de confusión, se pone de evidencia una tendencia clara. Cuando el modelo predice “Estrella”, su precisión es notablemente alta. En nuestro modelo con mejores resultados, el número de estrellas predichas correctamente fue de sólo 70 de 15.165, lo que arroja un índice de error sorprendentemente bajo del 0,46%. En cambio, en el caso de clases como “Galaxia”, el porcentaje de error es notablemente superior (3,297%), lo que significa que la tarea de clasificación es más difícil. Estos resultados arrojan información sobre los puntos fuertes y las complejidades de nuestros modelos predictivos.ConclusiónEn esta parte de nuestro viaje a través del análisis predictivo, hemos abarcado un terreno importante. Empezamos familiarizándonos con el conjunto de datos SDSS y su variada gama de características. Empleando el Local Outlier Factor (LOF) para la detección de valores atípicos, logramos resultados notables. A continuación, nos adentramos en un exhaustivo análisis de características, con la ayuda de matrices de correlación, para obtener información más detallada.Al abordar el problema del desequilibrio de datos, lo mitigamos hábilmente mediante técnicas de sobremuestreo. Estas acciones cruciales no sólo perfeccionaron nuestro conjunto de datos, sino que también nos prepararon para la última fase del modelado predictivo y la evaluación de modelos."
  },
  
  {
    "title": "Vino - Predecir la clase",
    "url": "/grupo6IA.github.io/posts/wine/",
    "categories": "Clasificación",
    "tags": "Data Cleaning, Data Visualization, Multiclass Classification",
    "date": "2023-10-04 00:00:00 -0300",
    





    
    "snippet": "Vino - Predecir la claseEn esta ocación se trata de un conjunto de datos sobre vinos donado en 1991 con el fin de probar algunas herramientas de AI, cuyo objetivo es determinar el origen de los vin...",
    "content": "Vino - Predecir la claseEn esta ocación se trata de un conjunto de datos sobre vinos donado en 1991 con el fin de probar algunas herramientas de AI, cuyo objetivo es determinar el origen de los vinos mediante un análisis químico. Con 178 instancias y 13 características, contiene los resultados de un análisis químico de vinos cultivados en la misma región de Italia pero derivados de tres viñedos diferentes. El análisis determinó las cantidades de 13 componentes presentes en cada uno de los tres tipos de vino. Originalmente, el conjunto de datos tenía unas 30 variables, pero, por alguna razón, sólo está disponible la versión de 13 dimensiones.Metadata dataset  Alcohol: El contenido de alcohol en el vino.  Malic Acid: La cantidad de ácido málico en el vino, un tipo de ácido orgánico.  Ash: Contenido de cenizas del vino.  Alcalinity of Ash: Alcalinidad de la ceniza del vino.  Magnesium: La cantidad de magnesio en el vino.  Total Phenols: El contenido total de fenoles en el vino.  Flavanoids: La concentración de flavonoides, un tipo de pigmento vegetal, en el vino.  Nonflavanoid Phenols: La concentración de compuestos fenólicos no flavonoides en el vino.  Proanthocyanins: La concentración de proantocianinas, un tipo de flavonoide, en el vino.  Color Intensity: La intensidad del color del vino.  Hue: La tonalidad o matiz de color del vino.  OD280/OD315 de vinos diluidos: La densidad óptica de los vinos diluidos, una medida de la densidad del color.  Proline: La concentración de prolina, un aminoácido, en el vino.Estos rasgos representan diversas características químicas de los vinos y pueden utilizarse para tareas de análisis y clasificación.Input DataEn primer lugar vamos a importar el conjunto de datos con algunas bibliotecas básicas, y ver si se cargó correctamente, y también dar una visión general de los datos con sus atributos.import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsdf = pd.read_csv(\"wine.csv\")X = df.drop('Class', axis=1)  y = df['Class'] df.head()  Click en la imagen para verla a pantalla completa.Data AnalysisUna vez que ya hemos comprobado que todo está correcto, podemos saltar a algunas características que tiene este conjunto de datos.#Statistical Analysis:df.describe()#Datatype Information:df.info()De estas tablas podemos extraer alguna información útil. En primer lugar, que este conjunto de datos no tiene valores faltantes, lo que facilita el trabajo. Además, podemos ver que los rangos de los diferentes atributos tienen muchas varianzas entre sí, esto podría ser un problema cuando entrenemos este conjunto porque características como “Magnesium” tendrán mucho peso en comparación con “Nonflavanoid phenols” o “Hue”. Podemos solucionar este problema utilizando un tipo de transformación, para saber qué transformación, debemos analizar la distribución de los datos.features = df.drop('Class', axis=1)fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(12, 10))fig.subplots_adjust(hspace=0.5)for i, col in enumerate(features.columns):    ax = axes[i // 4, i % 4]    features[col].hist(bins=25, color='blue', alpha=0.7, ax=ax)    ax.set_title(col)plt.show()Observando estos histogramas podemos ver que la mayoría de las características tienen una distribución gaussiana, por lo que la mejor opción es la transformación z. Además, aquí podemos ver que es posible que existan valores atípicos, algo que vamos a analizar más adelante.#Applying scaler = StandardScaler()X_normalized = scaler.fit_transform(features)X_normalized_df = pd.DataFrame(X_normalized, columns=features.columns)fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(12, 10))fig.subplots_adjust(hspace=0.5)for i, col in enumerate(X_normalized_df.columns):    ax = axes[i // 4, i % 4]    X_normalized_df[col].hist(bins=25, color='blue', alpha=0.7, ax=ax)    ax.set_title(col)plt.show()Ahora que los datos tienen una media de 0 y una desviación estándar de 1, ¡tendremos resultados mucho mejores!OutliersAhora es el momento de analizar si hay algún valor atípico, para ello vamos a utilizar algún boxplot para verlo gráficamente.#lets see whether our data has outliers or not:# create box plotsfig, ax = plt.subplots(ncols=5, nrows=3, figsize=(20,10))index = 0ax = ax.flatten()for col, value in X_normalized_df.items():    sns.boxplot(y=col, data=X_normalized_df, color='b', ax=ax[index])    index += 1plt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)Aquí podemos ver que hay algunos valores atípicos en: “Malid Acid”, “Ash”, “Alcalinity of ash”, entre otros. Pero tenemos que tener cuidado a la hora de elegir un umbral para los valores atípicos, si elegimos un valor para el umbral demasiado bajo (como 1 o 1,5) podemos eliminar una gran cantidad de datos, teniendo en cuenta que sólo tenemos 178 filas, tenemos que apuntar a un umbral más alto y eliminar aquellos valores atípicos que están más lejos de la mayoría de los datos o tienen valores significativamente más altos. El umbral elegido fue 2,7 con los siguientes resultados.def detect_outliers(column, threshold=2.7):    mean = np.mean(column)    std_deviation = np.std(column)    outliers = (column - mean).abs() &gt; threshold * std_deviation    return outlierscolumns_with_outliers = []for column in X_normalized_df.columns:    outliers = detect_outliers(X_normalized_df[column])    if outliers.any():        columns_with_outliers.append(column)        print(f\"Column '{column}' has {outliers.sum()} outliers.\")#data normalized and without outlierssns.pairplot(df_sin_outliers)Ahora que los datos están normalizados y se han eliminado los valores atípicos, podemos centrarnos más en la correlación de atributos.Correlated AttributesLos atributos que están correlacionados podrían marcar una gran diferencia en términos de precisión y rendimiento para muchos modelos, por eso vamos a centrarnos en este tema y específicamente en esta matriz de correlación.plt.figure(figsize=(15,10))sns.heatmap(df.corr(), annot=True, fmt='.2f', linewidths=2)Tenemos muchas cosas que analizar aquí. En primer lugar, centrándonos en el atributo ‘Ash’, podemos ver que tiene un peso de -0,05 en la variable objetivo, debido a esto podemos eliminar este atributo ya que es casi inútil para la etiqueta y mejorará el rendimiento. En segundo lugar, si nos centramos en los pesos de la etiqueta, podemos ver claramente que hay 3 atributos que tienen ±0,70 o más, lo que representa gran parte de la varianza de la etiqueta. Esos son “Total phenols”, “Flavanoids” y “OD280…”, la cuestión con estos atributos es que están fuertemente correlacionados entre sí (±0.70), esto podría ser un problema porque quizás están describiendo la misma cosa y no se necesitan 3 atributos. Después de pensar un rato, se me ocurrieron algunas soluciones.  Mi primera posibilidad fue intentar fusionar “Total Phenols” y “Flavanoids”.Como podemos ver, no sólo están correlacionadas entre sí, sino que además los valores de correlación de las demás variables son muy similares. Por ejemplo, tomando “Alcohol” como ejemplo, “Total Phenols” tiene una correlación de 0,29, mientras que “Flavanoides” tiene una correlación de 0,24. Esta tendencia se da con todos los atributos.Después de hacer algunas pruebas de fusión de estos dos (y el tercero más tarde) los resultados me muestran que no es una buena idea, ya que dio menos precisión que no hacer nada con este problema (ignorarlo). Por eso me lancé por mi segunda y tercera posibilidad.  Usar un tipo de reducción de dimensión.El primero que probé fue PCA, ya que he hecho algunos trabajos sobre el mismo y los resultados fueron exitosos. Utilicé el conjunto de datos sin valores atípicos y con los datos normalizados, pero ni siquiera eso fue útil, fue una mejora en comparación con mi primera posibilidad, pero no fue suficiente. Tal vez porque PCA borrado alguna información útil. (probado más adelante en el documento)Por eso decidí utilizar LDA (Linear Discriminant Analysis), una de las principales características de LDA es que maximiza la separación entre las diferentes clases en los datos, lo que la convierte en una técnica supervisada y con objetivo de encontrar las direcciones en las que las clases son más separables. Los resultados de utilizar LDA fueron claramente mucho mejores que los de las otras posibilidades.Model PredictionAhora es el momento de probar con diferentes modelos, con PCA, con LDA y sin nada.Random ForestEl primero es Random Forest, un algoritmo que combina varios árboles de decisión para mejorar la precisión y reducir el sobreajuste. Es una opción sólida para muchas aplicaciones debido a su capacidad para manejar características categóricas y numéricas, así como para tratar datos desequilibrados.clf = RandomForestClassifier(random_state=42)y = y[df_sin_outliers.index]# Perform cross-validation and get accuracy scores without outliersscores = cross_val_score(clf, df_sin_outliers, y, cv=5)mean_accuracy = scores.mean()print(\"Mean Accuracy with Random Forest after Removing Outliers:\", mean_accuracy)# PCA Dimensionality Reductionpca = PCA(n_components=2)X_pca = pca.fit_transform(df_sin_outliers)# Initialize Random Forest Classifier for PCA dataclfPCA = RandomForestClassifier(random_state=42)scoresPCA = cross_val_score(clfPCA, X_pca, y, cv=5)mean_accuracyPCA = scoresPCA.mean()print(\"Mean Accuracy with Random Forest after PCA Dimensionality Reduction:\", mean_accuracyPCA)# LDA Dimensionality Reductionlda = LinearDiscriminantAnalysis(n_components=2)X_lda = lda.fit_transform(df_sin_outliers, y)# Initialize Random Forest Classifier for LDA dataclf_rf = RandomForestClassifier(random_state=42)scores_rf_lda = cross_val_score(clf_rf, X_lda, y, cv=5)mean_accuracy_rf_lda = scores_rf_lda.mean()print(\"Mean Accuracy with Random Forest after LDA Dimensionality Reduction:\", mean_accuracy_rf_lda)SVCEl siguiente es SVC (Support Vector Classification). Se trata de un algoritmo versátil de aprendizaje automático utilizado para resolver problemas de clasificación binaria. Su objetivo principal es encontrar un límite de decisión óptimo que separe eficazmente los puntos de datos que pertenecen a clases diferentes, maximizando al mismo tiempo el margen entre ellos. El SVC es conocido por su capacidad para manejar patrones de datos no lineales y se emplea ampliamente en diversos ámbitos, lo que lo convierte en una herramienta fundamental en el campo del aprendizaje automático y la ciencia de datos.# Initialize Support Vector Classifier (SVC)clf_svc = SVC(random_state=42)y = y[df_sin_outliers.index]# Perform cross-validation and get accuracy scores without outliersscores_svc = cross_val_score(clf_svc, df_sin_outliers, y, cv=5)mean_accuracy_svc = scores_svc.mean()print(\"Mean Accuracy with SVC after Removing Outliers:\", mean_accuracy_svc)# PCA Dimensionality Reductionpca = PCA(n_components=2)X_pca = pca.fit_transform(df_sin_outliers)# Initialize SVC Classifier for PCA dataclf_svc_pca = SVC(random_state=42)scores_svc_pca = cross_val_score(clf_svc_pca, X_pca, y, cv=5)mean_accuracy_svc_pca = scores_svc_pca.mean()print(\"Mean Accuracy with SVC after PCA Dimensionality Reduction:\", mean_accuracy_svc_pca)# LDA Dimensionality Reductionlda = LinearDiscriminantAnalysis(n_components=2)X_lda = lda.fit_transform(df_sin_outliers, y)# Initialize SVC Classifier for LDA dataclf_svc_lda = SVC(random_state=42)scores_svc_lda = cross_val_score(clf_svc_lda, X_lda, y, cv=5)mean_accuracy_svc_lda = scores_svc_lda.mean()print(\"Mean Accuracy with SVC after LDA Dimensionality Reduction:\", mean_accuracy_svc_lda)KNNY por último, KNN (K-Nearest Neighbors). Funciona encontrando los k puntos de datos más cercanos a uno dado y haciendo predicciones basadas en su clase mayoritaria (para clasificación) o su media (para regresión). KNN es sencillo, no asume distribuciones de datos y se aplica ampliamente en el reconocimiento de patrones y el análisis de datos.# Initialize k-Nearest Neighbors (KNN) Classifierclf_knn = KNeighborsClassifier(n_neighbors=5)y = y[df_sin_outliers.index]# Perform cross-validation and get accuracy scores without outliersscores_knn = cross_val_score(clf_knn, df_sin_outliers, y, cv=5)mean_accuracy_knn = scores_knn.mean()print(\"Mean Accuracy with KNN after Removing Outliers:\", mean_accuracy_knn)# PCA Dimensionality Reductionpca = PCA(n_components=2)X_pca = pca.fit_transform(df_sin_outliers)# Initialize KNN Classifier for PCA dataclf_knn_pca = KNeighborsClassifier(n_neighbors=5)scores_knn_pca = cross_val_score(clf_knn_pca, X_pca, y, cv=5)mean_accuracy_knn_pca = scores_knn_pca.mean()print(\"Mean Accuracy with KNN after PCA Dimensionality Reduction:\", mean_accuracy_knn_pca)# LDA Dimensionality Reductionlda = LinearDiscriminantAnalysis(n_components=2)X_lda = lda.fit_transform(df_sin_outliers, y)# Initialize KNN Classifier for LDA dataclf_knn_lda = KNeighborsClassifier(n_neighbors=5)scores_knn_lda = cross_val_score(clf_knn_lda, X_lda, y, cv=5)mean_accuracy_knn_lda = scores_knn_lda.mean()print(\"Mean Accuracy with KNN after LDA Dimensionality Reduction:\", mean_accuracy_knn_lda)ConclusiónEste documento presenta un análisis exhaustivo del conjunto de datos Wine, comenzando con un examen minucioso de los datos y abordando las complejidades de los atributos correlacionados. A lo largo de nuestra exploración, hemos evaluado varias estrategias y, en última instancia, hemos llegado a la conclusión de que KNN y SVC son los modelos con mejores resultados. Sin embargo, debemos abordar estos resultados con cautela. Cabe destacar que LDA no falló en ninguna predicción, especialmente en modelos como SVC. Esto podría indicar que el modelo está sobreajustado. Aunque podría ser aceptable en este caso debido al tamaño y las características del conjunto de datos, en el futuro, para mantener el rendimiento del modelo, podría ser necesario un reentrenamiento para garantizar resultados más precisos y exactos. Sin embargo, hemos observado que la reducción de la dimensionalidad mediante LDA ha dado mejores resultados que otras técnicas alternativas."
  },
  
  {
    "title": "Titanic - Machine Learning from Disaster",
    "url": "/grupo6IA.github.io/posts/titanic/",
    "categories": "Clasificación",
    "tags": "Data Cleaning, Data Visualization, Binary Classification",
    "date": "2023-09-26 00:00:00 -0300",
    





    
    "snippet": "Titanic - Machine Learning from DisasterThe Titanic dataset presents an intriguing opportunity for data exploration, in this post we are going to dive into the preprocessing using Python. Our goal ...",
    "content": "Titanic - Machine Learning from DisasterThe Titanic dataset presents an intriguing opportunity for data exploration, in this post we are going to dive into the preprocessing using Python. Our goal is to get hands-on with the Titanic dataset, understand its ins and outs, and make informed predictions about whether passengers survived or not. As we navigate through this dataset, we’ll handle the data, create models, and unveil the intriguing tale of the Titanic.Metadata Dataset  Survived – Survival (0 = No; 1 = Yes)  PassengerId – Identification number for each person  Pclass – Passenger Class (1 = 1st ;  2 = 2nd ; 3 = 3rd )  Name – Passenger Name Sex – Sex (male of female)  SibSp – Number of Siblings/Spouses Aboard  Parch – Number of Parents/Children Aboard  Ticket – Ticket Number  Fare – Passenger Fare  Cabin – Passenger Cabin  Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)Input dataFirstly, we are going to import the two datasets, one for training called “train.csv” and other for testing called “test.csv”. This dataset has the advantage that already the data is divided in both separated csv.import numpy as np import pandas as pd import seaborn as snsfrom matplotlib import pyplot as pltsns.set_style(\"whitegrid\")import warningswarnings.filterwarnings(\"ignore\")pd.read_csv(\"train.csv\")pd.read_csv(\"test.csv\")To check that both sets were loaded correctly, we are going to use the the .head() function to print the first lines of the training csv.print(training.head())  Click the image to watch it fullscreen.Missing ValuesNow the problem to attack are the missing values, as you can see, the datasets could be loaded correctly and even some useful analytical statistics were shown for the future. But now we will have to analyze what these values are, how important they are for the label and, of the most important things, what quantity they are.As you can see, the column “Cabin” Bene many missing values, there are several ways to deal with these values, one of them is the imputation, using different methods to implement them, another method is to simply ignore the column, something that in some situations can be useful if there are too many missing values or it does not have much relevance in the objective variable.On the other hand, the column “Age” has much less quantity and it is a vital column for the label variable, in this case the imputation can be applied. But in the case of “Cabin” it is better not to take it into account. Here we also get rid of “Ticket” because it’s not giving us any information useful for the model, it’s only a identification number for the entry.training.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)testing.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)We already know that there are not many missing values in the “Age” column, but to be more precise in deciding what kind of imputation to do, we first have to look at the ranges and distribution of the data in the column.As you can see in the graph, the ages are a bit skewed to the right, so taking the mean may not be the best option, because as we know in these cases high values have a lot of weight in the mean. On the other hand, the median may be more accurate when replacing this missing values. Although it is known that it is not ideal to impute data, the median is the best option.training[\"Age\"].fillna(training[\"Age\"].median(), inplace = True)testing[\"Age\"].fillna(testing[\"Age\"].median(), inplace = True) training[\"Embarked\"].fillna(\"S\", inplace = True)testing[\"Fare\"].fillna(testing[\"Fare\"].median(), inplace = True)null_table(training, testing)Data VisualizationIt is important to analyze and visualize through graphs the data that we are going to use in our model, in this way we can find some important finding that can influence the model, for example, we can see how important are the attributes “Age” or “Sex” through which graphs. In other case perhaps it’s easier to use the correlative matrix to find this influence on the label, but here thanks to the understanding of the data I can know for sure that the age and sex of the person it’s very important for the label.As we can see the “Sex” column is very striking in terms of the results, and based on this graph we can draw the conclusion that this attribute is vital in predicting the label.sns.barplot(x=\"Pclass\", y=\"Survived\", data=training)plt.ylabel(\"Survival Rate\")plt.title(\"Distribution of Survival Based on Class\")plt.show()total_survived_one = training[training.Pclass == 1][\"Survived\"].sum()total_survived_two = training[training.Pclass == 2][\"Survived\"].sum()total_survived_three = training[training.Pclass == 3][\"Survived\"].sum()total_survived_class = total_survived_one + total_survived_two + total_survived_threeSimilar to the previous concept, we can see how class is also a key attribute in predicting whether or not you survived, since the large number of people who lived were in class number “1”, with a survival rate of just over 0.6.Analyzing more on other attributes such as “age”, we get these results:Based on these graphs we can analyze that age was another important factor in predicting whether a person survives or not, as we can see the younger the person is the more likely to survive.Adapting for ModelsBefore starting to apply any model for the datasets, we must first make sure that the data types of the columns are valid for certain models, in this case both the “Sex” and “Embarked” columns are categorical values, but for the classification model, it needs numerical values.In this case the female value of “Sex” is going to be 1, while male is going to be 0. While the “Embarked” column, the possible values are “S”, “C” and “Q”, in this case we are going to change them as 1, 2 and 3 respectively.Another problem is that both “SibSp” and “Parch” are quite similar attributes, something we can solve by making both columns join in a column called “Family Size”. At the same time we can make a new column that contains the information if the passenger was traveling alone or accompanied, a useful attribute thinking about the models used.Looking at the “Name” column, although it seems that there is no useful information in it, if we look at the title that the names have (such as “Mr” or “Ms”) can give us valuable information of the attribute. So what we can do is to delete the attribute “Name” and add a new column “Title” that simply says what title has each person without taking into account their real name, plus we would be transforming all the columns into numeric values (as it will be implemented as a dictionary where each title has an index).Normalize DataAnother problem to be solved comes from the “Age” and “Fare” columns since, as can be seen, the values vary greatly in range and this can cause inaccuracies when modeling. Therefore, the best thing to do is to normalize these attributes in a range from -1 to 1.Now the data with these changes will bring better results in the model as we lower the weight of the magnitudes!Model PredictionBefore getting into the model, we will first indicate which is the label and we will eliminate both the PassengerId and the label.A very easy mistake to make is to over-fit the models, as they would be too rigid to the training data, so it is useful to have a third dataset called validation test where we will make sure that we are not going to over-fit the data. To do this we will test many models with cross validation and verify which one is the most accurate.As can be seen the highest score was obtained by XGBoost followed by Random Forest and SVC, to predict we used the SVC model, issuing a csv with the results obtained by this model in the test dataset.results obtained by this model in the test dataset. This csv has 418 lines where the columns “PassengerId” and the objective variable “Survived” are located."
  }
  
]

