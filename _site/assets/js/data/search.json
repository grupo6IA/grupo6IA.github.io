[
  
  {
    "title": "Stellar Classification - SDSS17",
    "url": "/grupo6IA.github.io/posts/stellar/",
    "categories": "Classification",
    "tags": "Resampling, Data Visualization, Multiclass Classification",
    "date": "2023-10-09 00:00:00 -0300",
    





    
    "snippet": "Stellar Classification - SDSS17Since I was a child, one of the topics I’ve always been fascinated by is space. The vastness of the universe, the mysteries of the galaxies, and the beauty of the nig...",
    "content": "Stellar Classification - SDSS17Since I was a child, one of the topics I’ve always been fascinated by is space. The vastness of the universe, the mysteries of the galaxies, and the beauty of the night sky has always kept my curiosity. In the world of astronomy, one of the fundamental tasks is the classification of celestial objects, and the “Stellar Classification Dataset - SDSS17” provides an exciting opportunity to explore and understand the spectral characteristics of Stars, Galaxies, and Quasars. This dataset, based on observations from the Sloan Digital Sky Survey, contains a lot of information that allow us to classify these objects and delve into the wonders of our cosmos. Let’s embark on this astronomical journey and analyze this dataset.Metadata DatasetContext - SDSS17The Sloan Digital Sky Survey (SDSS), often referred to simply as SDSS, stands as a monumental achievement in the field of astronomy, a pioneer in unlocking the secrets of our vast cosmos. This multi-spectral imaging and spectroscopic redshift survey is conducted using a dedicated 2.5-meter wide-angle optical telescope located at the Apache Point Observatory in New Mexico, United States. Since its inception in the year 2000, SDSS has fundamentally transformed our understanding of the universe.At its core, SDSS was built upon groundbreaking instrumentation and data processing techniques. It utilized a multi-filter/multi-array scanning CCD camera to efficiently capture high-quality images of the night sky. These images were complemented by a multi-object/multi-fiber spectrograph capable of simultaneously obtaining spectra from numerous celestial objects.One of the project’s significant challenges was dealing with the unprecedented volume of data generated nightly by the telescope and instruments. This data was eventually processed, leading to the creation of extensive astronomical object catalogs, available for digital querying. SDSS played a pivotal role in advancing massive database storage and accessibility technologies, making its data widely accessible over the internet—a concept relatively novel at the time.The imaging camera collects photometric imaging data using an array of 30 SITe/Tektronix 2048 by 2048 pixel CCDs arranged in six columns of five CCDs each, aligned with the pixel columns of the CCDs themselves. SDSS r, i, u, z, and g filters cover the respective rows of the array, in that order. The survey operates the instrument in a drift scan mode: the camera slowly reads the CCDs as the data is being collected, while the telescope moves along great circles on the sky so that images of objects move along the columns of the CCDs at the same rate the CCDs are being read.Content - SDSS17The dataset comprises 100,000 observations of celestial objects captured by the Sloan Digital Sky Survey (SDSS). Each observation is characterized by 17 feature columns and 1 class column, which categorizes the object as either a star, galaxy, or quasar.Here’s a detailed description of the dataset columns:      obj_ID (Object Identifier): This is a unique identifier assigned to each object in the image catalog used by the CAS (Catalog Archive Server).        alpha (Right Ascension angle): The right ascension angle, measured at the J2000 epoch, specifies the east-west position of the object in the sky.        delta (Declination angle): The declination angle, measured at the J2000 epoch, specifies the north-south position of the object in the sky.        u (Ultraviolet filter): The u-filter corresponds to the ultraviolet portion of the photometric system and provides information about the object’s brightness in this range.        g (Green filter): The g-filter corresponds to the green portion of the photometric system and measures the object’s brightness in the green wavelength.        r (Red filter): The r-filter corresponds to the red portion of the photometric system and measures the object’s brightness in the red wavelength.        i (Near Infrared filter): The i-filter corresponds to the near-infrared portion of the photometric system and measures the object’s brightness in this range.        z (Infrared filter): The z-filter corresponds to the infrared portion of the photometric system and provides information about the object’s brightness in the infrared wavelength.        run_ID (Run Number): This is a unique identifier used to specify the particular scan during which the observation was made.        rereun_ID (Rerun Number): The rerun number specifies how the image was processed, indicating any reprocessing or modifications applied to the data.        cam_col (Camera Column): The camera column number identifies the specific scanline within the run.        field_ID (Field Number): The field number is used to identify each field, which is a specific region of the sky.        spec_obj_ID (Spectroscopic Object Identifier): This is a unique identifier for optical spectroscopic objects. Observations with the same spec_obj_ID share the same output class.        class (Object Class): This column categorizes the type of celestial object observed, including galaxies, stars, or quasars.        redshift: The redshift value is based on the increase in wavelength and is a critical measure for understanding the motion and distance of objects in the universe.        plate (Plate ID): The plate ID identifies each plate used in the SDSS. Plates are physical objects with holes drilled in them to allow fibers to collect light from celestial objects.        MJD (Modified Julian Date): MJD is used to indicate when a particular piece of SDSS data was collected, providing a precise timestamp for each observation.        fiber_ID (Fiber ID): Fiber ID identifies the specific fiber used to point light at the focal plane during each observation. This allows the SDSS to collect light from different objects simultaneously.  Input dataThe first step to make is import our libraries that we will use throughout the document, as well as check if it was loaded correctly printing the first lines of the dataset.import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.neighbors import LocalOutlierFactorfrom sklearn.preprocessing import LabelEncoderfrom imblearn.over_sampling import SMOTEfrom sklearn.linear_model import LogisticRegressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.preprocessing import MinMaxScalerfrom yellowbrick.classifier import ConfusionMatrixfrom lightgbm import LGBMClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import StratifiedKFoldfold = StratifiedKFold(n_splits=4, shuffle = True, random_state=42)from sklearn.metrics import recall_scorefrom warnings import filterwarningsfilterwarnings(action='ignore')df = pd.read_csv(\"star_classification.csv\")df.head()Here is some additional information about this dataset, rows, statistical information, among others.df.shapeAdjusting the labeldf.info()The observation at watching at this data is that the label it of type object, something we change it to numerical to be able to use it in models in the future.df[\"class\"]=[0 if i == \"GALAXY\" else 1 if i == \"STAR\" else 2 for i in df[\"class\"]]With this code we will change “GALAXY” to number 0, “STAR” to number 1, and “Quasars” to number 2. We can also see in the information table that in 100,000 rows no value is missing, an excellent piece of news.OutliersWhen we approach the outliers in such a big dataset, we have to be more careful, that’s why I decide to use a more robust approach, the outlier detection algorithm its Local Outlier Factor (LOF). The idea of LOF is to compare each data point to its nearby neighbors. It calculates how far each point is from its neighbors and checks if a point is much farther away from its neighbors than they are from each other. If it’s significantly farther, it’s considered an anomaly or outlier. If it’s not much farther, it’s considered a normal point. LOF uses this relative density comparison to find local anomalies in a dataset.clf = LocalOutlierFactor()y_pred = clf.fit_predict(df)x_score = clf.negative_outlier_factor_outlier_score = pd.DataFrame()outlier_score[\"score\"] = x_score#thresholdthreshold2 = -1.5filtre2 = outlier_score[\"score\"] &lt; threshold2outlier_index = outlier_score[filtre2].index.tolist()len(outlier_index)df.drop(outlier_index, inplace=True)df.shapeAfter deleting the outliers with LOF we get a total of 84744 total of rows in 18 features, now it’s time to take a closer look to the features.Data VisualizationLooking to the label we can see that the data is really unbalanced, almost the 60% of the rows are galaxies, and 19% are Quasars. This is a problem that we are going to attack in the future, at the moment of training a model, but for now we are going to take a look to the other features.Once we analyze a bit the label, now we can focus in the features, in this graph we analyze the distribution and density of each feature in relation to the target label. Although this graph is comprehensive, it provides valuable insights. On the left side, you can see the feature densities represented by Kernel Density Estimation (KDE) plots. These plots showcase how each feature’s distribution varies across three classes and provides an aggregated view across all classes. On the right side, histograms illustrate the same distributions. This visualization is powerful for assessing feature importance and linear relationships with the label.Future Selection# Select the features you want to visualize in the plotsfeatures = df.drop('class', axis=1)# Mapping of real labels to desired labelslabel_mapping = {0: 'Galaxy', 1: 'Star', 2: 'Quasar'}# Create a grid of subplotsfig, axes = plt.subplots(nrows=len(features.columns), ncols=2, figsize=(12, 2*len(features.columns)))fig.subplots_adjust(hspace=0.5)# Iterate through the features and generate plotsfor i, col in enumerate(features.columns):    # KDE Plot    ax1 = axes[i, 0]    for label_value, label_name in label_mapping.items():        sns.kdeplot(data=df[df[\"class\"] == label_value][col], label=label_name, ax=ax1)    sns.kdeplot(data=df[col], label=\"All\", ax=ax1)    ax1.set_title(f'{col} KDE')    ax1.legend()    # Histogram Plot    ax2 = axes[i, 1]    for label_value, label_name in label_mapping.items():        sns.histplot(data=df[df[\"class\"] == label_value][col], bins=25, color='blue', alpha=0.7, ax=ax2, label=label_name)    sns.histplot(data=df[col], bins=25, color='red', alpha=0.5, ax=ax2, label=\"All\")    ax2.set_title(f'{col} Histogram')    ax2.legend()plt.tight_layout()plt.show()Putting more focus on the density graphs, we have to analyze the linear relation with the label, for example, in “obj_id” we can see that for any value of “obj_id” the label hasn’t any variance. The density at different values of “obj_id” is always the same for the 3 classes (the 3 classes follow the same trend), from this we can take that this feature might be quite useless for the variance of the label.We can see something similar for “alpha” and “beta”, we know that these attributes define the location where the celestial object is in the space, and with these graphs we can say that the location of the celestial object isn’t an important feature to define if it’s a galaxy, star, or quasar.There are some features that follow this trend (“run_Id”, “cam_col”, “field_Id”, among others), some of them because there are just identifiers or not useful features for the model but important for some astrologers.But some features that we can clearly see that might be important for the label, for example: “u”, “g”, “r”, “i”, “z” and “plate”. For example, in case of “plate” we can see a lot of variances throughout the values of the plate, from values from 2000 to 4000 we can see that we have high probabilities that is a star, and from 7000 to 9500 might be a quasar. This can be seen in the features mention earlier.Looking to the correlation matrix we get the following results.As I said, some of the features that we already mention are the ones which are the more correlated, but “redshift” has the highest correlation among all the features, this means, that it has a lot of weight in the decision of the label with a 0.54, taking a closer look into this feature we get the following graph.  0 for Galaxy, 1 for Star, 2 for Quasar.Now that we take a closer look, we can clearly see the separation in density for different values of the redshift, from -15 to -5 we can see the density skyrocket for Star, whereas from -3 to -0.5 it seems to be a Galaxy, and for values exceeding 1, it’s probable that is a Quasar.In this other graph you can easily see the separation between the redshift value and the classes.Based on these graphs we can conclude that there is a strong linear correlation between the redshift and the labelFrom these graphs we concluded which features we are going to use on the models, we get rid of the others because there are ids or others because don’t have almost any correlation with the label.Dealing with unbalanced dataAs we observed from the earlier pie chart depicting the labels, it is evident that our dataset suffers from class imbalance. This graph provides a clearer representation of the same imbalance.For that reason, we need to resample the data. Now, we have two options: Oversampling and Undersampling. Oversampling involves increasing the number of samples in the minority class, which is the class with fewer examples, in order to balance the class distribution. This is typically done by duplicating or generating new samples for the minority class. On the other hand, Undersampling reduces the number of samples in the majority class, the class with more examples, with the same goal of achieving a balanced class distribution. Undersampling usually involves randomly removing samples from the majority class.One of the reasons why I opted for Oversampling over Undersampling is that I wanted to retain all of the data. Undersampling could certainly be considered in this scenario, as we have a substantial number of rows that we could potentially remove.In this case we are going to use SMOTE for the Oversampling methodsmote = SMOTE(random_state=42)X = df.drop('class', axis=1) y = df['class']X_resampled, y_resampled = smote.fit_resample(X, y)print(y_resampled.value_counts())y_resampled_df = pd.DataFrame({'class': y_resampled})plt.figure(figsize=(6, 4))sns.countplot(x='class', data=y_resampled_df)plt.title(\"Distribución de Clases después de SMOTE\")plt.xlabel(\"Clase\")plt.ylabel(\"Frecuencia\")plt.show()This are final dimensions of the dataset, without outliers, with the selected features, and with the oversample data.X = X_resampled[['u', 'g', 'r', 'i', 'z', 'redshift', 'plate']]X.shapeAs a critical first step before diving into the realm of machine learning, it’s essential to normalize our dataset. This foundational process ensures that all data features are effectively scaled, typically within a common range from 0 to 1. The reason behind this practice is to create a level playing field for machine learning models, preventing any single feature from unduly influencing the results. So, before we embark on our modeling journey, let’s make sure our data is on the same page.df=X.copy()scaler=MinMaxScaler()for i in ['u', 'g', 'r', 'i', 'z', 'redshift', 'plate']:    df[i]=scaler.fit_transform(df[[i]])df.head()   Model PredictionIn this section, we will focus on model prediction. We will be working with six different models, each chosen for its specific characteristics and suitability to our dataset and goals. Our objective is to thoroughly examine and evaluate the performance of these models to determine the most effective one for accurate predictions. This process is a crucial step in our analysis, and we will approach it with precision and objectivity.classes = ['GALAXY','STAR','QSO']X_train, X_test, y_train, y_test = train_test_split(df, y_resampled, test_size=0.3, random_state = 42)Naïve BayesTo kick off our analysis, we initiate the process with Naive Bayes as our first algorithm of choice. Naive Bayes provides an initial evaluation for our data exploration and prediction, setting the stage for a comprehensive evaluation of our modeling techniques.from sklearn.naive_bayes import GaussianNBmodelNB = GaussianNB()modelNB.fit(X_train, y_train)y_pred4 = modelNB.predict(X_test)gnb_score = recall_score(y_test, y_pred4, average = 'weighted')gnb_scoreNB_cm = ConfusionMatrix(modelNB, classes=classes, cmap='GnBu')NB_cm.fit(X_train, y_train)NB_cm.score(X_test, y_test)NB_cm.show()Logisitc RegressionFollowing our initial exploration with Naive Bayes, we proceed to implement Logistic Regression as the second algorithm in our analysis. Logistic Regression offers a different perspective and approach to our predictive modeling, enhancing the comprehensiveness of our evaluation and providing valuable insights into our dataset.modelLR = LogisticRegression(max_iter=1000)modelLR.fit(X_train, y_train)y_pred1 = modelLR.predict(X_test)from sklearn.metrics import recall_scoreLR_score = recall_score(y_test, y_pred1, average='weighted')print(LR_score)NB_cm = ConfusionMatrix(modelNB, classes=classes, cmap='GnBu')NB_cm.fit(X_train, y_train)NB_cm.score(X_test, y_test)NB_cm.show()KNNAs we continue our journey through the world of predictive modeling, our next destination is K-Nearest Neighbors, often referred to as KNN. KNN is our third algorithm choice and offers a unique approach to understanding our data and making predictions. This transition allows us to explore a different facet of our dataset and expand our modeling horizons.modelknn = KNeighborsClassifier(n_neighbors = 1)modelknn.fit(X_train, y_train)y_pred2 = modelknn.predict(X_test)knn_score = recall_score(y_test, y_pred2, average='weighted')print(knn_score)knn_cm = ConfusionMatrix(modelknn, classes=classes, cmap='GnBu')knn_cm.fit(X_train, y_train)knn_cm.score(X_test, y_test)knn_cm.show()Decision TreeAs we progress in our predictive modeling journey, our next destination is the Decision Tree algorithm. Representing the fourth step in our analysis, Decision Tree introduces a tree-like structure that simplifies complex decision-making. This transition provides us with a structured approach to unraveling patterns within our dataset, offering a clear and interpretable pathway for making predictions.from sklearn.tree import DecisionTreeClassifiermodelDT = DecisionTreeClassifier(random_state = 30)modelDT.fit(X_train, y_train)y_pred3 = modelDT.predict(X_test)dtree_score = recall_score(y_test, y_pred3, average='weighted')print(dtree_score)DT_cm = ConfusionMatrix(modelDT, classes=classes, cmap='GnBu')DT_cm.fit(X_train, y_train)DT_cm.score(X_test, y_test)DT_cm.show()Random ForestAs we delve deeper into the world of predictive modeling, our next stop is the Random Forest algorithm. Random Forest, the fifth pick in our toolbox, takes an ensemble approach by uniting multiple decision trees to offer a powerful blend of robustness and precision. With this shift, we aim to tap into the collective wisdom of these trees, supercharging our predictive abilities and uncovering more insights within our dataset.from sklearn.ensemble import RandomForestClassifiermodelRF = RandomForestClassifier(n_estimators = 19, random_state = 30)modelRF.fit(X_train, y_train)y_pred5 = modelRF.predict(X_test)from sklearn.metrics import recall_scorerf_score = recall_score(y_test, y_pred5, average = 'weighted')print(rf_score)RF_cm = ConfusionMatrix(modelRF, classes=classes, cmap='GnBu')RF_cm.fit(X_train, y_train)RF_cm.score(X_test, y_test)RF_cm.show()XGBoostAs we step into the prediction game, let’s shine the spotlight on XGBoost as our sixth and final algorithm. XGBoost is famous for its gradient boosting technique, which injects some serious predictive mojo into our analysis. This boosts our capabilities and lets us dive even deeper into our dataset.import xgboost as xgbmodelXG = xgb.XGBClassifier(random_state = 42)modelXG.fit(X_train, y_train)y_pred6 = modelXG.predict(X_test)from sklearn.metrics import recall_scorexgb_score = recall_score(y_test, y_pred6, average = 'weighted')print(xgb_score)XG_cm = ConfusionMatrix(modelXG, classes=classes, cmap='GnBu')XG_cm.fit(X_train, y_train)XG_cm.score(X_test, y_test)XG_cm.show()ResultsFrom our thorough performance analysis, including percentages and the confusion matrix, we’ve gathered some insightful findings. Notably, XGBoost and Random Forest emerge as the top-performing models, showcasing remarkable predictive power. However, it’s equally remarkable that even the ‘underdog,’ Naïve Bayes, achieved a commendable accuracy rate of 92%, which is quite impressive!Delving into the nuances revealed by the confusion matrix, a distinct trend becomes apparent. When the model is predicting ‘Star,’ its accuracy is remarkably high. In our best-performing model, the number of stars correctly predicted was a mere 70 out of 15,165, resulting in a strikingly low error ratio of 0.46%. On the other hand, for classes such as ‘Galaxy,’ the error ratio is notably higher at 3.297%, signifying a more challenging classification task. These findings shed light on the strengths and intricacies of our predictive models.ConclusionIn this part of our journey through predictive analysis, we’ve covered significant ground. We began by acquainting ourselves with the SDSS dataset and its diverse array of features. Employing the Local Outlier Factor (LOF) for outlier detection, we achieved remarkable results. Subsequently, we delved into a comprehensive feature analysis, aided by correlation matrices, to gain deeper insights.Addressing the challenge of data imbalance, we skillfully mitigated it through oversampling techniques. This laid the foundation for our in-depth exploration, utilizing six distinct algorithms to navigate the augmented dataset.These crucial actions not only refined our dataset but also prepared us for the last stage of predictive modeling and model evaluation."
  },
  
  {
    "title": "Wine - Predicting Class",
    "url": "/grupo6IA.github.io/posts/wine/",
    "categories": "Classification",
    "tags": "Data Cleaning, Data Visualization, Multiclass Classification",
    "date": "2023-10-04 00:00:00 -0300",
    





    
    "snippet": "Wine - Predicting ClassThis is a wine dataset donated in 1991 with the purpose of testing some IA tools, it has the objective of determinate the origin of the wines using a chemical analysis. With ...",
    "content": "Wine - Predicting ClassThis is a wine dataset donated in 1991 with the purpose of testing some IA tools, it has the objective of determinate the origin of the wines using a chemical analysis. With 178 instances and 13 features it contains the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. Originally, the dataset had around 30 variables, but for some reason, only the 13-dimensional version is available. Unfortunately, the list of the 30 variables is lost, and it’s unclear which 13 variables are included in this version.Metadata dataset  Alcohol: The alcohol content in the wine.  Malic Acid: The amount of malic acid in the wine, a type of organic acid.  Ash: The ash content in the wine.  Alcalinity of Ash: The alkalinity of the ash in the wine.  Magnesium: The amount of magnesium in the wine.  Total Phenols: The total phenolic content in the wine.  Flavanoids: The concentration of flavonoids, a type of plant pigment, in the wine.  Nonflavanoid Phenols: The concentration of non-flavonoid phenolic compounds in the wine.  Proanthocyanins: The concentration of proanthocyanins, a type of flavonoid, in the wine.  Color Intensity: The intensity of color in the wine.  Hue: The hue or color shade of the wine.  OD280/OD315 of Diluted Wines: The optical density of diluted wines, a measure of color density.  Proline: The concentration of proline, an amino acid, in the wine.These features represent various chemical characteristics of the wines and can be used for analysis and classification tasks.Input DataFirst of all we are going to import the dataset with some basic libraries, and see if it was loaded correctly, also give an overview of the data with their attributes.import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsdf = pd.read_csv(\"wine.csv\")X = df.drop('Class', axis=1)  y = df['Class'] df.head()  Click the image to watch it fullscreen.Data AnalysisOnce we already verify that all things all correct, we can jump on some characteristics that this dataset have.#Statistical Analysis:df.describe()#Datatype Information:df.info()From this tables we can extract some useful information. Firstly, that this dataset hasn’t have any missing values, so that makes the job easier. Additionally, we can see that the ranges of the different attributes have a lot of variances between each other, this might be a problem when we train this set because features such as “Magnesium” will have a lot of weight compared to “Nonflavanoid phenols” or “Hue”. We can solve this problem by using a type of transformation, to know which transformation, we must analyze the distribution of the data.features = df.drop('Class', axis=1)fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(12, 10))fig.subplots_adjust(hspace=0.5)for i, col in enumerate(features.columns):    ax = axes[i // 4, i % 4]    features[col].hist(bins=25, color='blue', alpha=0.7, ax=ax)    ax.set_title(col)plt.show()Looking at these histograms we can we that most of the features have a Gaussian distribution, therefore the best option is the z-transformation. Additionally here we can see that it is possible the existence of outliers, something that we are going to analyze further on.#Applying scaler = StandardScaler()X_normalized = scaler.fit_transform(features)X_normalized_df = pd.DataFrame(X_normalized, columns=features.columns)fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(12, 10))fig.subplots_adjust(hspace=0.5)for i, col in enumerate(X_normalized_df.columns):    ax = axes[i // 4, i % 4]    X_normalized_df[col].hist(bins=25, color='blue', alpha=0.7, ax=ax)    ax.set_title(col)plt.show()Now that the data have mean of 0 and standard deviation of 1, we will have much better results!OutliersNow it’s time to analyze if there are any outliers, for that we are going to use some boxplot to view it graphically.#lets see whether our data has outliers or not:# create box plotsfig, ax = plt.subplots(ncols=5, nrows=3, figsize=(20,10))index = 0ax = ax.flatten()for col, value in X_normalized_df.items():    sns.boxplot(y=col, data=X_normalized_df, color='b', ax=ax[index])    index += 1plt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)Here we can see that there are some outliers in: “Malid Acid”, “Ash”, “Alcalinity of ash”, between others. But we have to be careful at moment of choosing a threshold for the outliers, if we choose a value for the threshold to low (like 1 or 1.5) we can delete a big amount of data, considering that we only have 178 rows, we have to aim for a higher threshold and remove those outliers that are further away from the majority of the data or have significantly higher values. The threshold chosen was 2.7 with the following results.def detect_outliers(column, threshold=2.7):    mean = np.mean(column)    std_deviation = np.std(column)    outliers = (column - mean).abs() &gt; threshold * std_deviation    return outlierscolumns_with_outliers = []for column in X_normalized_df.columns:    outliers = detect_outliers(X_normalized_df[column])    if outliers.any():        columns_with_outliers.append(column)        print(f\"Column '{column}' has {outliers.sum()} outliers.\")#data normalized and without outlierssns.pairplot(df_sin_outliers)Now that the data is normalized and the outliers were deleted, we can focus more on the correlation of attributes.Correlated AttributesAttributes that are correlated could make a big difference in terms of accuracy and performance for a lot of models, that’s why we are going to focus in this topic and specifically in this correlated matrix.plt.figure(figsize=(15,10))sns.heatmap(df.corr(), annot=True, fmt='.2f', linewidths=2)We have a lot of things to analyze here. Firstly, focusing on the attribute ‘Ash,’ we can see that it has a weight of -0.05 on the label, due to this we can delete this attribute since its almost useless for the label and it will improve the performance. Secondly, if we focus on weights of the label, we can clearly see that there are 3 attributes that are ±0.70 or more, which represent a lot of the variance of the label. Those are “Total phenols”, “Flavanoids” and “OD280…”, the thing with these attributes is that there are strongly correlated with each other (±0.70), this could be a problem because perhaps there are describing the same thing and is not needed 3 attributes. After thinking for a while, I came up with some solutions.  My first possibility was trying to merge “Total Phenols” and “Flavanoids”As we can see not only are they correlated with each other, but also the correlation values of the other variables are very similar. For example, taking “Alcohol” as an example, “Total Phenols” has a correlation of 0.29 while “Flavanoids” has a correlation of 0.24. This trend is with all attributes.After making some testing merging this two (and the third later) the results show me that isn’t a good idea because it gave less accuracy that not doing anything with this problem (ignore it). That’s why I jumped for my second and third possibility.  Using a type of dimension reduction.The first one that I tried was PCA, since I have already made some work on it and the results were successful. I used the dataset without outliers and with the normalize data, but even that wasn’t useful, it was an improve compared to my first possibility, but it wasn’t enough. Perhaps because PCA deleted some useful information. (tested later on the document)That’s why I decide using LDA (Linear Discriminant Analysis), one of the main characteristic of LDA is that it maximize separation between different classes in the data, which makes it a supervised technique and its goal is to find the directions in which classes are most separable. The results of using LDA were clearly much better than the other possibilities.Model PredictionNow it’s time to test with different models, with PCA, with LDA and without anything.Random ForestThe first one its Random Forest, which it’s an ensemble algorithm that combines multiple decision trees to improve accuracy and reduce overfitting. It’s a solid choice for many applications due to its ability to handle both categorical and numerical features, as well as dealing with imbalanced data.clf = RandomForestClassifier(random_state=42)y = y[df_sin_outliers.index]# Perform cross-validation and get accuracy scores without outliersscores = cross_val_score(clf, df_sin_outliers, y, cv=5)mean_accuracy = scores.mean()print(\"Mean Accuracy with Random Forest after Removing Outliers:\", mean_accuracy)# PCA Dimensionality Reductionpca = PCA(n_components=2)X_pca = pca.fit_transform(df_sin_outliers)# Initialize Random Forest Classifier for PCA dataclfPCA = RandomForestClassifier(random_state=42)scoresPCA = cross_val_score(clfPCA, X_pca, y, cv=5)mean_accuracyPCA = scoresPCA.mean()print(\"Mean Accuracy with Random Forest after PCA Dimensionality Reduction:\", mean_accuracyPCA)# LDA Dimensionality Reductionlda = LinearDiscriminantAnalysis(n_components=2)X_lda = lda.fit_transform(df_sin_outliers, y)# Initialize Random Forest Classifier for LDA dataclf_rf = RandomForestClassifier(random_state=42)scores_rf_lda = cross_val_score(clf_rf, X_lda, y, cv=5)mean_accuracy_rf_lda = scores_rf_lda.mean()print(\"Mean Accuracy with Random Forest after LDA Dimensionality Reduction:\", mean_accuracy_rf_lda)SVCThe next one its SVC (Support Vector Classification). It is a versatile machine learning algorithm used for solving binary classification problems. Its primary objective is to find an optimal decision boundary that effectively separates data points belonging to different classes while maximizing the margin between them. SVC is known for its ability to handle non-linear data patterns and is widely employed in various domains, making it a fundamental tool in the field of machine learning and data science.# Initialize Support Vector Classifier (SVC)clf_svc = SVC(random_state=42)y = y[df_sin_outliers.index]# Perform cross-validation and get accuracy scores without outliersscores_svc = cross_val_score(clf_svc, df_sin_outliers, y, cv=5)mean_accuracy_svc = scores_svc.mean()print(\"Mean Accuracy with SVC after Removing Outliers:\", mean_accuracy_svc)# PCA Dimensionality Reductionpca = PCA(n_components=2)X_pca = pca.fit_transform(df_sin_outliers)# Initialize SVC Classifier for PCA dataclf_svc_pca = SVC(random_state=42)scores_svc_pca = cross_val_score(clf_svc_pca, X_pca, y, cv=5)mean_accuracy_svc_pca = scores_svc_pca.mean()print(\"Mean Accuracy with SVC after PCA Dimensionality Reduction:\", mean_accuracy_svc_pca)# LDA Dimensionality Reductionlda = LinearDiscriminantAnalysis(n_components=2)X_lda = lda.fit_transform(df_sin_outliers, y)# Initialize SVC Classifier for LDA dataclf_svc_lda = SVC(random_state=42)scores_svc_lda = cross_val_score(clf_svc_lda, X_lda, y, cv=5)mean_accuracy_svc_lda = scores_svc_lda.mean()print(\"Mean Accuracy with SVC after LDA Dimensionality Reduction:\", mean_accuracy_svc_lda)KNNAnd lastly, KNN (K-Nearest Neighbors). It works by finding the k closest data points to a given one and making predictions based on their majority class (for classification) or their average (for regression). KNN is simple, doesn’t assume data distributions, and is widely applied in pattern recognition and data analysis.# Initialize k-Nearest Neighbors (KNN) Classifierclf_knn = KNeighborsClassifier(n_neighbors=5)y = y[df_sin_outliers.index]# Perform cross-validation and get accuracy scores without outliersscores_knn = cross_val_score(clf_knn, df_sin_outliers, y, cv=5)mean_accuracy_knn = scores_knn.mean()print(\"Mean Accuracy with KNN after Removing Outliers:\", mean_accuracy_knn)# PCA Dimensionality Reductionpca = PCA(n_components=2)X_pca = pca.fit_transform(df_sin_outliers)# Initialize KNN Classifier for PCA dataclf_knn_pca = KNeighborsClassifier(n_neighbors=5)scores_knn_pca = cross_val_score(clf_knn_pca, X_pca, y, cv=5)mean_accuracy_knn_pca = scores_knn_pca.mean()print(\"Mean Accuracy with KNN after PCA Dimensionality Reduction:\", mean_accuracy_knn_pca)# LDA Dimensionality Reductionlda = LinearDiscriminantAnalysis(n_components=2)X_lda = lda.fit_transform(df_sin_outliers, y)# Initialize KNN Classifier for LDA dataclf_knn_lda = KNeighborsClassifier(n_neighbors=5)scores_knn_lda = cross_val_score(clf_knn_lda, X_lda, y, cv=5)mean_accuracy_knn_lda = scores_knn_lda.mean()print(\"Mean Accuracy with KNN after LDA Dimensionality Reduction:\", mean_accuracy_knn_lda)ConclusionThis document presents a comprehensive analysis of the Wine dataset, commencing with a thorough examination of the data and addressing the intricacies of correlated attributes. Throughout our exploration, we have evaluated various strategies, ultimately discerning that KNN and SVC emerged as the top-performing models. However, we must approach these results with caution. It’s noteworthy that LDA didn’t miss any predictions, particularly in models like SVC. This could be an indication of an overfitting model. While it might be acceptable in this instance due to the dataset’s size and features, in the future, to sustain the model’s performance, retraining might be necessary to ensure more precise and accurate results. But we observed that dimensionality reduction using LDA yielded superior results when compared to alternative techniques."
  },
  
  {
    "title": "Titanic - Machine Learning from Disaster",
    "url": "/grupo6IA.github.io/posts/titanic/",
    "categories": "Classification",
    "tags": "Data Cleaning, Data Visualization, Binary Classification",
    "date": "2023-09-26 00:00:00 -0300",
    





    
    "snippet": "Titanic - Machine Learning from DisasterThe Titanic dataset presents an intriguing opportunity for data exploration, in this post we are going to dive into the preprocessing using Python. Our goal ...",
    "content": "Titanic - Machine Learning from DisasterThe Titanic dataset presents an intriguing opportunity for data exploration, in this post we are going to dive into the preprocessing using Python. Our goal is to get hands-on with the Titanic dataset, understand its ins and outs, and make informed predictions about whether passengers survived or not. As we navigate through this dataset, we’ll handle the data, create models, and unveil the intriguing tale of the Titanic.Metadata Dataset  Survived – Survival (0 = No; 1 = Yes)  PassengerId – Identification number for each person  Pclass – Passenger Class (1 = 1st ;  2 = 2nd ; 3 = 3rd )  Name – Passenger Name Sex – Sex (male of female)  SibSp – Number of Siblings/Spouses Aboard  Parch – Number of Parents/Children Aboard  Ticket – Ticket Number  Fare – Passenger Fare  Cabin – Passenger Cabin  Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)Input dataFirstly, we are going to import the two datasets, one for training called “train.csv” and other for testing called “test.csv”. This dataset has the advantage that already the data is divided in both separated csv.import numpy as np import pandas as pd import seaborn as snsfrom matplotlib import pyplot as pltsns.set_style(\"whitegrid\")import warningswarnings.filterwarnings(\"ignore\")pd.read_csv(\"train.csv\")pd.read_csv(\"test.csv\")To check that both sets were loaded correctly, we are going to use the the .head() function to print the first lines of the training csv.print(training.head())  Click the image to watch it fullscreen.Missing ValuesNow the problem to attack are the missing values, as you can see, the datasets could be loaded correctly and even some useful analytical statistics were shown for the future. But now we will have to analyze what these values are, how important they are for the label and, of the most important things, what quantity they are.As you can see, the column “Cabin” Bene many missing values, there are several ways to deal with these values, one of them is the imputation, using different methods to implement them, another method is to simply ignore the column, something that in some situations can be useful if there are too many missing values or it does not have much relevance in the objective variable.On the other hand, the column “Age” has much less quantity and it is a vital column for the label variable, in this case the imputation can be applied. But in the case of “Cabin” it is better not to take it into account. Here we also get rid of “Ticket” because it’s not giving us any information useful for the model, it’s only a identification number for the entry.training.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)testing.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)We already know that there are not many missing values in the “Age” column, but to be more precise in deciding what kind of imputation to do, we first have to look at the ranges and distribution of the data in the column.As you can see in the graph, the ages are a bit skewed to the right, so taking the mean may not be the best option, because as we know in these cases high values have a lot of weight in the mean. On the other hand, the median may be more accurate when replacing this missing values. Although it is known that it is not ideal to impute data, the median is the best option.training[\"Age\"].fillna(training[\"Age\"].median(), inplace = True)testing[\"Age\"].fillna(testing[\"Age\"].median(), inplace = True) training[\"Embarked\"].fillna(\"S\", inplace = True)testing[\"Fare\"].fillna(testing[\"Fare\"].median(), inplace = True)null_table(training, testing)Data VisualizationIt is important to analyze and visualize through graphs the data that we are going to use in our model, in this way we can find some important finding that can influence the model, for example, we can see how important are the attributes “Age” or “Sex” through which graphs. In other case perhaps it’s easier to use the correlative matrix to find this influence on the label, but here thanks to the understanding of the data I can know for sure that the age and sex of the person it’s very important for the label.As we can see the “Sex” column is very striking in terms of the results, and based on this graph we can draw the conclusion that this attribute is vital in predicting the label.sns.barplot(x=\"Pclass\", y=\"Survived\", data=training)plt.ylabel(\"Survival Rate\")plt.title(\"Distribution of Survival Based on Class\")plt.show()total_survived_one = training[training.Pclass == 1][\"Survived\"].sum()total_survived_two = training[training.Pclass == 2][\"Survived\"].sum()total_survived_three = training[training.Pclass == 3][\"Survived\"].sum()total_survived_class = total_survived_one + total_survived_two + total_survived_threeSimilar to the previous concept, we can see how class is also a key attribute in predicting whether or not you survived, since the large number of people who lived were in class number “1”, with a survival rate of just over 0.6.Analyzing more on other attributes such as “age”, we get these results:Based on these graphs we can analyze that age was another important factor in predicting whether a person survives or not, as we can see the younger the person is the more likely to survive.Adapting for ModelsBefore starting to apply any model for the datasets, we must first make sure that the data types of the columns are valid for certain models, in this case both the “Sex” and “Embarked” columns are categorical values, but for the classification model, it needs numerical values.In this case the female value of “Sex” is going to be 1, while male is going to be 0. While the “Embarked” column, the possible values are “S”, “C” and “Q”, in this case we are going to change them as 1, 2 and 3 respectively.Another problem is that both “SibSp” and “Parch” are quite similar attributes, something we can solve by making both columns join in a column called “Family Size”. At the same time we can make a new column that contains the information if the passenger was traveling alone or accompanied, a useful attribute thinking about the models used.Looking at the “Name” column, although it seems that there is no useful information in it, if we look at the title that the names have (such as “Mr” or “Ms”) can give us valuable information of the attribute. So what we can do is to delete the attribute “Name” and add a new column “Title” that simply says what title has each person without taking into account their real name, plus we would be transforming all the columns into numeric values (as it will be implemented as a dictionary where each title has an index).Normalize DataAnother problem to be solved comes from the “Age” and “Fare” columns since, as can be seen, the values vary greatly in range and this can cause inaccuracies when modeling. Therefore, the best thing to do is to normalize these attributes in a range from -1 to 1.Now the data with these changes will bring better results in the model as we lower the weight of the magnitudes!Model PredictionBefore getting into the model, we will first indicate which is the label and we will eliminate both the PassengerId and the label.A very easy mistake to make is to over-fit the models, as they would be too rigid to the training data, so it is useful to have a third dataset called validation test where we will make sure that we are not going to over-fit the data. To do this we will test many models with cross validation and verify which one is the most accurate.As can be seen the highest score was obtained by XGBoost followed by Random Forest and SVC, to predict we used the SVC model, issuing a csv with the results obtained by this model in the test dataset.results obtained by this model in the test dataset. This csv has 418 lines where the columns “PassengerId” and the objective variable “Survived” are located."
  }
  
]

